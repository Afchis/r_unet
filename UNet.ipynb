{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['images_val', 'labels_val', 'test', 'labels', 'images']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import scipy.ndimage.morphology as morph\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "to_pil = transforms.ToPILImage()\n",
    "os.listdir('../r_unet/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "cell_model = {\n",
    "    'Rnn' : ConvRnnCell(in_channels, out_channels), \n",
    "    'Gru' : ConvGruCell(in_channels, out_channels), \n",
    "    'Rrn' : ConvRrnCell(in_channels, out_channels), \n",
    "    'Sru' : ConvSruCell(in_channels, out_channels), \n",
    "    'Dru' : ConvDruCell(in_channels, out_channels)\n",
    "}\n",
    "\"\"\"\n",
    "RECURRENT = False\n",
    "PARAMETERS = {\n",
    "    'd1':False, \n",
    "    'd2':False, \n",
    "    'd3':False, \n",
    "    'b_':False, \n",
    "    'u1':False, \n",
    "    'u2':False, \n",
    "    'u3':False, \n",
    "    'cell_model':'Rnn'\n",
    "}\n",
    "\n",
    "DEVICE = \"cuda:1\"\n",
    "# arguments\n",
    "TIMESTEPS = 3\n",
    "BATCH_SIZE = 1\n",
    "NUM_EPOCHS = 200\n",
    "INPUT_SIZE = 128\n",
    "INPUT_CHANNELS = 1\n",
    "NUM_CLASSES = 2\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                              transforms.Resize((INPUT_SIZE, INPUT_SIZE), interpolation = 0),\n",
    "                              transforms.ToTensor()\n",
    "                              ])\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "# decive\n",
    "device = torch.device(DEVICE if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# way to the data folders\n",
    "FOLDER_DATA = \"../r_unet/data/images\"\n",
    "FOLDER_MASK = \"../r_unet/data/labels\"\n",
    "FOLDER_TEST = \"../r_unet/data/test\"\n",
    "FOLDER_DATA_VAL = \"../r_unet/data/images_val\"\n",
    "FOLDER_MASK_VAL = \"../r_unet/data/labels_val\"\n",
    "\n",
    "FILE_NAMES = sorted(os.listdir('../r_unet/data/images'))\n",
    "FILE_NAMES_VAL = sorted(os.listdir('../r_unet/data/images_val'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(object):\n",
    "    label1 = (object==0).float()\n",
    "    label2 = (label1==0).float()\n",
    "    labels = torch.stack([label1, label2], dim=1).squeeze()\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainMedData(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.time = TIMESTEPS\n",
    "        self.folder_data = FOLDER_DATA\n",
    "        self.folder_mask = FOLDER_MASK\n",
    "        self.file_names = FILE_NAMES\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        gif_list = []\n",
    "        for i in range(self.time):\n",
    "            gif_list.append(transform(Image.open(self.folder_data + '/' + self.file_names[idx+i])))\n",
    "        gif_data = torch.stack(gif_list)\n",
    "        gif_list.clear()\n",
    "        for i in range(self.time):\n",
    "            gif_list.append(get_labels(transform(Image.open(self.folder_mask + '/' + self.file_names[idx+i]))))\n",
    "        gif_mask = torch.stack(gif_list)\n",
    "        gif_list.clear()\n",
    "        for i in range(self.time):\n",
    "            img = Image.open(self.folder_mask + '/' + self.file_names[idx+i])\n",
    "            img = img.resize((INPUT_SIZE, INPUT_SIZE), resample=Image.NEAREST)\n",
    "            gif_list.append(to_tensor(morph.distance_transform_edt(np.asarray(img)/255)))\n",
    "        gif_depth = torch.stack(gif_list)\n",
    "        return gif_data, gif_mask, gif_depth\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_names) - self.time + 1\n",
    "\n",
    "\n",
    "class ValMedData(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.time = TIMESTEPS\n",
    "        self.folder_data = FOLDER_DATA_VAL\n",
    "        self.folder_mask = FOLDER_MASK_VAL\n",
    "        self.file_names = FILE_NAMES_VAL\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        gif_list = []\n",
    "        for i in range(self.time):\n",
    "            gif_list.append(transform(Image.open(self.folder_data + '/' + self.file_names[idx+i])))\n",
    "        gif_data = torch.stack(gif_list)\n",
    "        gif_list.clear()\n",
    "        for i in range(self.time):\n",
    "            gif_list.append(get_labels(transform(Image.open(self.folder_mask + '/' + self.file_names[idx+i]))))\n",
    "        gif_mask = torch.stack(gif_list).squeeze(dim=2)\n",
    "        gif_list.clear()\n",
    "        for i in range(self.time):\n",
    "            img = Image.open(self.folder_mask + '/' + self.file_names[idx+i])\n",
    "            img = img.resize((INPUT_SIZE, INPUT_SIZE), resample=Image.NEAREST)\n",
    "            gif_list.append(to_tensor(morph.distance_transform_edt(np.asarray(img)/255)))\n",
    "        gif_depth = torch.stack(gif_list)\n",
    "        return gif_data, gif_mask, gif_depth\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names) - self.time + 1\n",
    "\n",
    "\n",
    "class TestMedData(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__\n",
    "        self.time = TIMESTEPS\n",
    "        self.folder_test = FOLDER_TEST\n",
    "        self.file_names = FILE_NAMES + FILE_NAMES_VAL\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        gif_list = []\n",
    "        for i in range(self.time):\n",
    "            gif_list.append(transform(Image.open(self.folder_test + '/' + self.file_names[idx+i])))\n",
    "        gif_test = torch.stack(gif_list)\n",
    "        gif_list.clear()\n",
    "        return gif_test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names) - self.time + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TrainMedData(Dataset):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.time = TIMESTEPS\n",
    "#         self.folder_data = FOLDER_DATA\n",
    "#         self.folder_mask = FOLDER_MASK\n",
    "#         self.file_names = FILE_NAMES\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         gif_list = []\n",
    "#         for i in range(self.time):\n",
    "#             gif_list.append(transform(Image.open(self.folder_data + '/' + self.file_names[idx+i])))\n",
    "#         gif_data = torch.stack(gif_list)\n",
    "#         gif_list.clear()\n",
    "#         for i in range(self.time):\n",
    "#             gif_list.append(get_labels(transform(Image.open(self.folder_mask + '/' + self.file_names[idx+i]))))\n",
    "#             depth_list = morph.distance_transform_edt(gif_list)\n",
    "#         gif_mask = torch.stack(gif_list)\n",
    "#         gif_depth = torch.stack(depth_list)\n",
    "#         gif_list.clear()\n",
    "#         depth_list.clear()\n",
    "\n",
    "#         return gif_data, gif_mask, gif_depth\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.file_names) - self.time + 1\n",
    "\n",
    "\n",
    "# class ValMedData(Dataset):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.time = TIMESTEPS\n",
    "#         self.folder_data = FOLDER_DATA_VAL\n",
    "#         self.folder_mask = FOLDER_MASK_VAL\n",
    "#         self.file_names = FILE_NAMES_VAL\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         gif_list = []\n",
    "#         for i in range(self.time):\n",
    "#             gif_list.append(transform(Image.open(self.folder_data + '/' + self.file_names[idx+i])))\n",
    "#         gif_data = torch.stack(gif_list)\n",
    "#         gif_list.clear()\n",
    "#         for i in range(self.time):\n",
    "#             gif_list.append(get_labels(transform(Image.open(self.folder_mask + '/' + self.file_names[idx+i]))))\n",
    "#         gif_mask = torch.stack(gif_list).squeeze(dim=2)\n",
    "#         gif_list.clear()\n",
    "#         for i in range(self.time):\n",
    "#             img = Image.open(self.folder_mask + '/' + self.file_names[idx+i])\n",
    "#             img = img.resize((INPUT_SIZE, INPUT_SIZE), resample=Image.NEAREST)\n",
    "#             gif_list.append(to_tensor(morph.distance_transform_edt(np.asarray(img)/255)))\n",
    "#         gif_depth = torch.stack(gif_list)\n",
    "#         return gif_data, gif_mask, gif_depth\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.file_names) - self.time + 1\n",
    "\n",
    "\n",
    "# class TestMedData(Dataset):\n",
    "#     def __init__(self):\n",
    "#         super().__init__\n",
    "#         self.time = TIMESTEPS\n",
    "#         self.folder_test = FOLDER_TEST\n",
    "#         self.file_names = FILE_NAMES + FILE_NAMES_VAL\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         gif_list = []\n",
    "#         for i in range(self.time):\n",
    "#             gif_list.append(transform(Image.open(self.folder_test + '/' + self.file_names[idx+i])))\n",
    "#         gif_test = torch.stack(gif_list)\n",
    "#         gif_list.clear()\n",
    "#         return gif_test\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.file_names) - self.time + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrainMedData()\n",
    "valid_dataset = ValMedData()\n",
    "test_dataset = TestMedData()\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          num_workers=1,\n",
    "                          shuffle=False)\n",
    "\n",
    "valid_loader = DataLoader(dataset=valid_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          num_workers=1,\n",
    "                          shuffle=False)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=1,\n",
    "                         num_workers=1,\n",
    "                         shuffle=False)\n",
    "\n",
    "data_loaders = {\n",
    "    'train' : train_loader,\n",
    "    'valid' : valid_loader,\n",
    "    'test' : test_loader\n",
    "}\n",
    "\n",
    "dataset_sizes = {\n",
    "    'train': len(train_dataset),\n",
    "    'valid': len(valid_dataset),\n",
    "    'test': len(test_dataset)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, d = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_pil((depth!=0).float())\n",
    "# depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y, d = train_dataset[0]\n",
    "# depth = d[0]\n",
    "# for dim_0 in depth:\n",
    "#     for dim_1 in dim_0:\n",
    "#         Dim_2 = torch.tensor([])\n",
    "#         for dim_2 in dim_1:\n",
    "#             if dim_2 == 0:\n",
    "#                 dim_2 = torch.ones_like(dim_2)\n",
    "#             torch.cat(dim_2)\n",
    "# print(Dim_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvRnnCell(nn.Module):    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvRnnCell, self).__init__()\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(in_channels+out_channels, out_channels, kernel_size=3, padding=1))\n",
    "             \n",
    "    def forward(self, x, hidden):\n",
    "        out = torch.cat([x, hidden],dim=1)\n",
    "        out = self.conv1(out)\n",
    "        hidden = out\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvGruCell(nn.Module):    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvGruCell, self).__init__()\n",
    "        self.conv_for_input = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        \n",
    "        self.conv_for_hidden = nn.Sequential(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))\n",
    "        \n",
    "        self.conv_2x_update = nn.Sequential(nn.Conv2d(in_channels+out_channels, out_channels, kernel_size=3, padding=1))\n",
    "        self.conv_2x_reset = nn.Sequential(nn.Conv2d(in_channels+out_channels, out_channels, kernel_size=3, padding=1))\n",
    "        \n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "     \n",
    "    def forward(self, x, hidden):\n",
    "        input = torch.cat([x, hidden],dim=1)\n",
    "\n",
    "        update_gate = self.conv_2x_update(input)\n",
    "        update_gate = self.sig((update_gate)) ### output after update gate\n",
    "        reset_gate = self.conv_2x_reset(input)\n",
    "        reset_gate = self.sig((reset_gate)) ### output after reset gate\n",
    "        \n",
    "        \n",
    "        memory_for_input = self.conv_for_input(x)\n",
    "        memory_for_hidden = self.conv_for_hidden(hidden)# просто хидден\n",
    "\n",
    "        memory_content = memory_for_input + (reset_gate * memory_for_hidden) ### output for reset gate(affects how the reset gate do work)\n",
    "        memory_content = self.relu(memory_content)\n",
    "\n",
    "        hidden = (update_gate * hidden) + ((1 - update_gate) * memory_content) # torch.ones(input_size, hidden_size)\n",
    "\n",
    "        return hidden, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvRrnCell(nn.Module):    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvRrnCell, self).__init__()\n",
    "        self.conv_for_input = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        \n",
    "        self.conv_for_hidden = nn.Sequential(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))\n",
    "        \n",
    "        self.conv_2x_update = nn.Sequential(nn.Conv2d(in_channels+out_channels, out_channels, kernel_size=3, padding=1))\n",
    "        self.conv_2x_reset = nn.Sequential(nn.Conv2d(in_channels+out_channels, out_channels, kernel_size=3, padding=1))\n",
    "        \n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "     \n",
    "    def forward(self, x, hidden):\n",
    "        input = torch.cat([x, hidden],dim=1)\n",
    "\n",
    "        update_gate = self.conv_2x_update(input)\n",
    "        update_gate = self.sig((update_gate)) \n",
    "        \n",
    "        memory_for_input = self.conv_for_input(x)\n",
    "        memory_for_hidden = hidden\n",
    "\n",
    "        memory_content = memory_for_input + memory_for_hidden\n",
    "        memory_content = self.relu(memory_content)\n",
    "\n",
    "        hidden = (update_gate * hidden) + ((1 - update_gate) * memory_content) # torch.ones(input_size, hidden_size)\n",
    "\n",
    "        return hidden, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvSruCell(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvSruCell, self).__init__()\n",
    "        self.update_gate = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.backbone = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        z = self.update_gate(x)\n",
    "        z = self.sig(z)\n",
    "        \n",
    "        h_ = self.backbone(x)\n",
    "        h_ = self.tanh(h_)\n",
    "        \n",
    "        h_prev = hidden * z\n",
    "        h = (1 - z) * h_\n",
    "        out = h + h_prev\n",
    "        return out, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDruCell(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvDruCell, self).__init__()\n",
    "        self.update_gate = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.reset_gate = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n",
    "        self.backbone = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        z = self.update_gate(x)\n",
    "        z = self.sig(z)\n",
    "        \n",
    "        r = self.reset_gate(x)\n",
    "        r = self.sig(r)\n",
    "        rx = r * x\n",
    "        h_ = self.backbone(rx)\n",
    "        h_ = self.tanh(h_)\n",
    "        \n",
    "        h_prev = hidden * z\n",
    "        h = (1 - z) * h_\n",
    "        out = h + h_prev\n",
    "        return out, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvRnn(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, ConvRnn_input_size, cell_model, reccurent=RECURRENT): # arg for ConvRnn layer\n",
    "        super(ConvRnn, self).__init__()\n",
    "        self.cell_dict = {\n",
    "            'Rnn' : ConvRnnCell(in_channels, out_channels), \n",
    "            'Gru' : ConvGruCell(in_channels, out_channels), \n",
    "            'Rrn' : ConvRrnCell(in_channels, out_channels), \n",
    "            'Sru' : ConvSruCell(in_channels, out_channels), \n",
    "            'Dru' : ConvDruCell(in_channels, out_channels)\n",
    "        }\n",
    "        self.rec = reccurent\n",
    "        self.cell_model = cell_model\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.timesteps = TIMESTEPS\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.input_size = ConvRnn_input_size\n",
    "        self.hidden_size = (self.batch_size, self.out_channels, self.input_size, self.input_size)\n",
    "        \n",
    "        self.ConvRnn_layer = self.cell_dict[self.cell_model]\n",
    "        self.init_hidden = torch.zeros(self.hidden_size).to(device)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_cells = None\n",
    "        x_list = []\n",
    "\n",
    "        x = x.reshape(self.batch_size, self.timesteps, self.in_channels, self.input_size, self.input_size)\n",
    "        x = x.permute(1, 0, 2, 3, 4)\n",
    "        if self.rec == True:\n",
    "            for i in range(self.timesteps):\n",
    "                if x_cells is None:\n",
    "                    x_cells, hidden = self.ConvRnn_layer(x[i], self.init_hidden)\n",
    "                    x_list.append(x_cells)\n",
    "                else:\n",
    "                    x_i, hidden = self.ConvRnn_layer(x[i], hidden)\n",
    "                    x_list.append(x_i)\n",
    "        elif self.rec == False:\n",
    "            for i in range(self.timesteps):\n",
    "                if x_cells is None:\n",
    "                    x_cells, _ = self.ConvRnn_layer(x[i], self.init_hidden)\n",
    "                    x_list.append(x_cells)\n",
    "                else:\n",
    "                    x_i, _ = self.ConvRnn_layer(x[i], self.init_hidden)\n",
    "                    x_list.append(x_i)\n",
    "        else:\n",
    "            print('RECURRENT can be only True or False')\n",
    "            quit()\n",
    "        x_cells = torch.stack(x_list)\n",
    "        x_cells = x_cells.permute(1, 0, 2, 3, 4)\n",
    "\n",
    "        x_cells = x_cells.reshape(-1, self.out_channels, self.input_size, self.input_size)\n",
    "        return x_cells  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvRnnRelu(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels ,ConvRnn_input_size, cell_model):\n",
    "        super(ConvRnnRelu, self).__init__()\n",
    "        self.cell_model = cell_model\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.input_size = ConvRnn_input_size\n",
    "        self.convrnnrelu = nn.Sequential(ConvRnn(self.in_channels, self.out_channels, \n",
    "                                                 self.input_size, self.cell_model),\n",
    "                                         nn.ReLU()\n",
    "                                         )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.convrnnrelu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvRelu(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvRelu, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.convrelu = nn.Sequential(nn.Conv2d(self.in_channels, self.out_channels, kernel_size=3, padding=1),\n",
    "                                      nn.ReLU()\n",
    "                                      )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.convrelu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaxPool, self).__init__()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.maxpool(x)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpAndCat(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(UpAndCat, self).__init__()\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "\n",
    "    def forward(self, x_up, x_cat):\n",
    "        out = self.up(x_up)\n",
    "        out = torch.cat([out, x_cat], dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetDesigner(nn.Module):    \n",
    "    def __init__(self, d1, d2, d3, b_, u1, u2, u3, cell_model,\n",
    "                 input_size=INPUT_SIZE, input_channels=INPUT_CHANNELS, num_classes=NUM_CLASSES):\n",
    "        super(UNetDesigner, self).__init__()\n",
    "        self.cell_model = cell_model\n",
    "        self.num_classes = NUM_CLASSES\n",
    "        self.d1, self.d2, self.d3, self.b, self.u1, self.u2, self.u3 = d1, d2, d3, b_, u1, u2, u3\n",
    "        self.input_size = input_size\n",
    "        self.input_chennels = input_channels\n",
    "        self.ch_list = [self.input_chennels, 32, 64, 128, 256]\n",
    "        self.input_x2 = int(self.input_size / 2)\n",
    "        self.input_x4 = int(self.input_size / 4)\n",
    "        self.input_x8 = int(self.input_size / 8)\n",
    "\n",
    "         ##### Down_1 layer ##### input_size = 128\n",
    "        if self.d1 == True:\n",
    "            self.down1 = nn.Sequential(ConvRnnRelu(self.ch_list[0], self.ch_list[1],    # Channels\n",
    "                                                   self.input_size, self.cell_model),               \n",
    "                                       ConvRelu(self.ch_list[1], self.ch_list[1])                   \n",
    "                                       )                                                # 1  -->32\n",
    "        else:                                                                           # 32 -->32\n",
    "            self.down1 = nn.Sequential(ConvRelu(self.ch_list[0], self.ch_list[1]),\n",
    "                                       ConvRelu(self.ch_list[1], self.ch_list[1])\n",
    "                                       )\n",
    "        self.down1_pool = MaxPool()\n",
    "\n",
    "         ##### Down_2 layer ##### input_size = 64\n",
    "        if self.d2 == True:\n",
    "            self.down2 = nn.Sequential(ConvRnnRelu(self.ch_list[1], self.ch_list[2],\n",
    "                                                   self.input_x2, self.cell_model),\n",
    "                                       ConvRelu(self.ch_list[2], self.ch_list[2])\n",
    "                                       )                                                # 32 -->64\n",
    "        else:                                                                           # 64 -->64\n",
    "            self.down2 = nn.Sequential(ConvRelu(self.ch_list[1], self.ch_list[2]),\n",
    "                                       ConvRelu(self.ch_list[2], self.ch_list[2])\n",
    "                                       )\n",
    "        self.down2_pool = MaxPool()\n",
    "\n",
    "         ##### Down_3 layer ##### input_size = 32\n",
    "        if self.d3 == True:\n",
    "            self.down3 = nn.Sequential(ConvRnnRelu(self.ch_list[2], self.ch_list[3], \n",
    "                                                   self.input_x4, self.cell_model),\n",
    "                                       ConvRelu(self.ch_list[3], self.ch_list[3])\n",
    "                                       )                                                # 64 -->128\n",
    "        else:                                                                           # 128-->128\n",
    "            self.down3 = nn.Sequential(ConvRelu(self.ch_list[2], self.ch_list[3]),\n",
    "                                       ConvRelu(self.ch_list[3], self.ch_list[3])\n",
    "                                       )\n",
    "        self.down3_pool = MaxPool()\n",
    "\n",
    "         ##### Bottom layer ##### input_size = 16\n",
    "        if self.b == True:\n",
    "            self.bottom = nn.Sequential(ConvRnnRelu(self.ch_list[3], self.ch_list[4], \n",
    "                                                    self.input_x8, self.cell_model),\n",
    "                                        ConvRelu(self.ch_list[4], self.ch_list[4])\n",
    "                                        )                                               # 128-->256\n",
    "        else:                                                                           # 256-->256\n",
    "            self.bottom = nn.Sequential(ConvRelu(self.ch_list[3], self.ch_list[4]),\n",
    "                                        ConvRelu(self.ch_list[4], self.ch_list[4])\n",
    "                                        )\n",
    "\n",
    "         ##### Up_3 layer #####\n",
    "        self.up_cat_3 = UpAndCat()\n",
    "        if self.u3 == True:\n",
    "            self.up_conv_3 = nn.Sequential(ConvRnnRelu(self.ch_list[4]+self.ch_list[3], \n",
    "                                                       self.ch_list[3], \n",
    "                                                       self.input_x4, self.cell_model),\n",
    "                                           ConvRelu(self.ch_list[3], self.ch_list[3])\n",
    "                                           )                                            # 394-->128\n",
    "        else:                                                                           # 128-->128\n",
    "            self.up_conv_3 = nn.Sequential(ConvRelu(self.ch_list[4]+self.ch_list[3], \n",
    "                                                    self.ch_list[3]),\n",
    "                                           ConvRelu(self.ch_list[3], self.ch_list[3])\n",
    "                                           )\n",
    "\n",
    "         ##### Up_2 layer #####\n",
    "        self.up_cat_2 = UpAndCat()\n",
    "        if self.u2 == True:\n",
    "            self.up_conv_2 = nn.Sequential(ConvRnnRelu(self.ch_list[3]+self.ch_list[2], \n",
    "                                                       self.ch_list[2], \n",
    "                                                       self.input_x2, self.cell_model),\n",
    "                                           ConvRelu(self.ch_list[2], self.ch_list[2])\n",
    "                                           )                                            # 192-->64\n",
    "        else:                                                                           # 64 -->64\n",
    "            self.up_conv_2 = nn.Sequential(ConvRelu(self.ch_list[3]+self.ch_list[2], \n",
    "                                                    self.ch_list[2]),\n",
    "                                           ConvRelu(self.ch_list[2], self.ch_list[2])\n",
    "                                           )\n",
    "\n",
    "         ##### Up_1 layer #####\n",
    "        self.up_cat_1 = UpAndCat()\n",
    "        if self.u1 == True:\n",
    "            self.up_conv_1 = nn.Sequential(ConvRnnRelu(self.ch_list[2]+self.ch_list[1], \n",
    "                                                       self.ch_list[1], \n",
    "                                                       self.input_size, self.cell_model),\n",
    "                                           ConvRelu(self.ch_list[1], self.ch_list[1])\n",
    "                                           )                                            # 96 -->32\n",
    "        else:                                                                           # 32 -->32\n",
    "            self.up_conv_1 = nn.Sequential(ConvRelu(self.ch_list[2]+self.ch_list[1], \n",
    "                                                    self.ch_list[1]),\n",
    "                                           ConvRelu(self.ch_list[1], self.ch_list[1])\n",
    "                                           )\n",
    "\n",
    "         ##### Final layer #####\n",
    "        self.final = nn.Sequential(nn.Conv2d(self.ch_list[1], self.num_classes, kernel_size=1),\n",
    "\n",
    "                                   )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, self.input_chennels, self.input_size, self.input_size)\n",
    "        # print(x.shape)\n",
    "        down1_feat = self.down1(x)\n",
    "        pool1 = self.down1_pool(down1_feat)\n",
    "        # print(pool1.shape)\n",
    "        down2_feat = self.down2(pool1)\n",
    "        pool2 = self.down2_pool(down2_feat)\n",
    "        # print(pool2.shape)\n",
    "        down3_feat = self.down3(pool2)\n",
    "        pool3 = self.down3_pool(down3_feat)\n",
    "        # print(pool3.shape)\n",
    "        bottom_feat = self.bottom(pool3)\n",
    "        # print(bottom_feat.shape)\n",
    "        up_feat3 = self.up_cat_3(bottom_feat, down3_feat)\n",
    "        up_feat3 = self.up_conv_3(up_feat3)\n",
    "        \n",
    "        up_feat2 = self.up_cat_2(up_feat3, down2_feat)\n",
    "        up_feat2 = self.up_conv_2(up_feat2)\n",
    "        \n",
    "        up_feat1 = self.up_cat_1(up_feat2, down1_feat)\n",
    "        up_feat1 = self.up_conv_1(up_feat1)\n",
    "        \n",
    "        out = self.final(up_feat1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNetDesigner(d1=PARAMETERS['d1'], \n",
    "                     d2=PARAMETERS['d2'], \n",
    "                     d3=PARAMETERS['d3'], \n",
    "                     b_=PARAMETERS['b_'], \n",
    "                     u1=PARAMETERS['u1'], \n",
    "                     u2=PARAMETERS['u2'], \n",
    "                     u3=PARAMETERS['u3'], \n",
    "                     cell_model=PARAMETERS['cell_model']\n",
    "                     )\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "def bce(x, y):\n",
    "    y = y.reshape(x.shape)\n",
    "    return F.binary_cross_entropy_with_logits(x, y)\n",
    "\n",
    "def l2_norm(x, y):\n",
    "    #d = torch.cat([d, d], dim=2)\n",
    "    #d = d.reshape(x.shape)\n",
    "    y = y.reshape(x.shape)\n",
    "    out = ((x - y)**2).sum()\n",
    "    #print('out.shape', out)\n",
    "    #print('d.shape', d.shape)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** epoch:  0 **********\n",
      "train l2_norm:  0.6506718072024259\n",
      "val l2_norm:  0.5107154349486033\n",
      "********** epoch:  1 **********\n",
      "train l2_norm:  0.5216057259928096\n",
      "val l2_norm:  0.47798484067122143\n",
      "********** epoch:  2 **********\n",
      "train l2_norm:  0.49305016886104236\n",
      "val l2_norm:  0.4489508271217346\n",
      "********** epoch:  3 **********\n",
      "train l2_norm:  0.44975514303554187\n",
      "val l2_norm:  0.4313349922498067\n",
      "********** epoch:  4 **********\n",
      "train l2_norm:  0.409607243808833\n",
      "val l2_norm:  0.3759111513694127\n",
      "********** epoch:  5 **********\n",
      "train l2_norm:  0.3860013484954834\n",
      "val l2_norm:  0.3780222286780675\n",
      "********** epoch:  6 **********\n",
      "train l2_norm:  0.36322118070992554\n",
      "val l2_norm:  0.36862223347028095\n",
      "********** epoch:  7 **********\n",
      "train l2_norm:  0.3499299612912265\n",
      "val l2_norm:  0.3370284785827001\n",
      "********** epoch:  8 **********\n",
      "train l2_norm:  0.3488766713575883\n",
      "val l2_norm:  0.36606477200984955\n",
      "********** epoch:  9 **********\n",
      "train l2_norm:  0.3437944447452372\n",
      "val l2_norm:  0.35099872946739197\n",
      "********** epoch:  10 **********\n",
      "train l2_norm:  0.33998517014763574\n",
      "val l2_norm:  0.3471532464027405\n",
      "********** epoch:  11 **********\n",
      "train l2_norm:  0.3360278538682244\n",
      "val l2_norm:  0.34525907039642334\n",
      "********** epoch:  12 **********\n",
      "train l2_norm:  0.3387594751336358\n",
      "val l2_norm:  0.3412842055161794\n",
      "********** epoch:  13 **********\n",
      "train l2_norm:  0.32807259261608124\n",
      "val l2_norm:  0.3412781010071437\n",
      "********** epoch:  14 **********\n",
      "train l2_norm:  0.3291584023020484\n",
      "val l2_norm:  0.3471691558758418\n",
      "********** epoch:  15 **********\n",
      "train l2_norm:  0.3241496262225238\n",
      "val l2_norm:  0.3477066308259964\n",
      "********** epoch:  16 **********\n",
      "train l2_norm:  0.3227101943709634\n",
      "val l2_norm:  0.3437379052241643\n",
      "********** epoch:  17 **********\n",
      "train l2_norm:  0.3179569068280133\n",
      "val l2_norm:  0.31986987590789795\n",
      "********** epoch:  18 **********\n",
      "train l2_norm:  0.3165868493643674\n",
      "val l2_norm:  0.3323935220638911\n",
      "********** epoch:  19 **********\n",
      "train l2_norm:  0.3099737939509479\n",
      "val l2_norm:  0.31946612397829693\n",
      "********** epoch:  20 **********\n",
      "train l2_norm:  0.3053763630715283\n",
      "val l2_norm:  0.32789597908655804\n",
      "********** epoch:  21 **********\n",
      "train l2_norm:  0.3149707452817397\n",
      "val l2_norm:  0.34497928619384766\n",
      "********** epoch:  22 **********\n",
      "train l2_norm:  0.3163235593925823\n",
      "val l2_norm:  0.3230475038290024\n",
      "********** epoch:  23 **********\n",
      "train l2_norm:  0.3084539540789344\n",
      "val l2_norm:  0.32897623876730603\n",
      "********** epoch:  24 **********\n",
      "train l2_norm:  0.3069525239142505\n",
      "val l2_norm:  0.34347735345363617\n",
      "********** epoch:  25 **********\n",
      "train l2_norm:  0.3080746531486511\n",
      "val l2_norm:  0.3176940580209096\n",
      "********** epoch:  26 **********\n",
      "train l2_norm:  0.3045359673825177\n",
      "val l2_norm:  0.316556915640831\n",
      "********** epoch:  27 **********\n",
      "train l2_norm:  0.3024781793355942\n",
      "val l2_norm:  0.3220662623643875\n",
      "********** epoch:  28 **********\n",
      "train l2_norm:  0.31196319379589776\n",
      "val l2_norm:  0.31425391634305316\n",
      "********** epoch:  29 **********\n",
      "train l2_norm:  0.304069768298756\n",
      "val l2_norm:  0.3249632865190506\n",
      "********** epoch:  30 **********\n",
      "train l2_norm:  0.3031232235106555\n",
      "val l2_norm:  0.3296816945075989\n",
      "********** epoch:  31 **********\n",
      "train l2_norm:  0.30457997051152313\n",
      "val l2_norm:  0.3125345508257548\n",
      "********** epoch:  32 **********\n",
      "train l2_norm:  0.2982532165267251\n",
      "val l2_norm:  0.31125178436438244\n",
      "********** epoch:  33 **********\n",
      "train l2_norm:  0.2948152639649131\n",
      "val l2_norm:  0.36322688559691113\n",
      "********** epoch:  34 **********\n",
      "train l2_norm:  0.3076485422524539\n",
      "val l2_norm:  0.30986568828423816\n",
      "********** epoch:  35 **********\n",
      "train l2_norm:  0.29645550521937286\n",
      "val l2_norm:  0.30166488885879517\n",
      "********** epoch:  36 **********\n",
      "train l2_norm:  0.3000874885103919\n",
      "val l2_norm:  0.31787917017936707\n",
      "********** epoch:  37 **********\n",
      "train l2_norm:  0.2969768467274579\n",
      "val l2_norm:  0.3131457070509593\n",
      "********** epoch:  38 **********\n",
      "train l2_norm:  0.2950281405990774\n",
      "val l2_norm:  0.3104258328676224\n",
      "********** epoch:  39 **********\n",
      "train l2_norm:  0.29191136360168457\n",
      "val l2_norm:  0.3058301657438278\n",
      "********** epoch:  40 **********\n",
      "train l2_norm:  0.29359892823479394\n",
      "val l2_norm:  0.31454725563526154\n",
      "********** epoch:  41 **********\n",
      "train l2_norm:  0.2916570969603278\n",
      "val l2_norm:  0.32089708745479584\n",
      "********** epoch:  42 **********\n",
      "train l2_norm:  0.29457257010719995\n",
      "val l2_norm:  0.317135492960612\n",
      "********** epoch:  43 **********\n",
      "train l2_norm:  0.2917892418124459\n",
      "val l2_norm:  0.30239340166250867\n",
      "********** epoch:  44 **********\n",
      "train l2_norm:  0.28739753433249215\n",
      "val l2_norm:  0.30780736605326336\n",
      "********** epoch:  45 **********\n",
      "train l2_norm:  0.2877418371764096\n",
      "val l2_norm:  0.3093045304218928\n",
      "********** epoch:  46 **********\n",
      "train l2_norm:  0.28755839643153275\n",
      "val l2_norm:  0.305505911509196\n",
      "********** epoch:  47 **********\n",
      "train l2_norm:  0.28578954325480893\n",
      "val l2_norm:  0.30467087030410767\n",
      "********** epoch:  48 **********\n",
      "train l2_norm:  0.28928612849929114\n",
      "val l2_norm:  0.32079436381657916\n",
      "********** epoch:  49 **********\n",
      "train l2_norm:  0.2914122275330804\n",
      "val l2_norm:  0.32105537752310437\n",
      "********** epoch:  50 **********\n",
      "train l2_norm:  0.28867013075134973\n",
      "val l2_norm:  0.30053524672985077\n",
      "********** epoch:  51 **********\n",
      "train l2_norm:  0.28260029378262436\n",
      "val l2_norm:  0.3011934558550517\n",
      "********** epoch:  52 **********\n",
      "train l2_norm:  0.2821717438372699\n",
      "val l2_norm:  0.3033486803372701\n",
      "********** epoch:  53 **********\n",
      "train l2_norm:  0.28224085813218897\n",
      "val l2_norm:  0.30260948340098065\n",
      "********** epoch:  54 **********\n",
      "train l2_norm:  0.2800228941169652\n",
      "val l2_norm:  0.29713209966818493\n",
      "********** epoch:  55 **********\n",
      "train l2_norm:  0.2821580787951296\n",
      "val l2_norm:  0.30009304483731586\n",
      "********** epoch:  56 **********\n",
      "train l2_norm:  0.27987549928101624\n",
      "val l2_norm:  0.3199380536874135\n",
      "********** epoch:  57 **********\n",
      "train l2_norm:  0.2807166962461038\n",
      "val l2_norm:  0.3006875862677892\n",
      "********** epoch:  58 **********\n",
      "train l2_norm:  0.2788058560002934\n",
      "val l2_norm:  0.31443163255850476\n",
      "********** epoch:  59 **********\n",
      "train l2_norm:  0.2783304181965915\n",
      "val l2_norm:  0.2997448941071828\n",
      "********** epoch:  60 **********\n",
      "train l2_norm:  0.27772900665348227\n",
      "val l2_norm:  0.2936657667160034\n",
      "********** epoch:  61 **********\n",
      "train l2_norm:  0.273734294555404\n",
      "val l2_norm:  0.29491647084554035\n",
      "********** epoch:  62 **********\n",
      "train l2_norm:  0.27272963523864746\n",
      "val l2_norm:  0.2894241015116374\n",
      "********** epoch:  63 **********\n",
      "train l2_norm:  0.2700571289116686\n",
      "val l2_norm:  0.28920600314935047\n",
      "********** epoch:  64 **********\n",
      "train l2_norm:  0.2691042572259903\n",
      "val l2_norm:  0.28777332107226056\n",
      "********** epoch:  65 **********\n",
      "train l2_norm:  0.26745937493714417\n",
      "val l2_norm:  0.28853632013003033\n",
      "********** epoch:  66 **********\n",
      "train l2_norm:  0.2671700506047769\n",
      "val l2_norm:  0.28476277987162274\n",
      "********** epoch:  67 **********\n",
      "train l2_norm:  0.264577331868085\n",
      "val l2_norm:  0.2937241941690445\n",
      "********** epoch:  68 **********\n",
      "train l2_norm:  0.26305061307820404\n",
      "val l2_norm:  0.2851507365703583\n",
      "********** epoch:  69 **********\n",
      "train l2_norm:  0.2621383849870075\n",
      "val l2_norm:  0.2804961105187734\n",
      "********** epoch:  70 **********\n",
      "train l2_norm:  0.26049024205316196\n",
      "val l2_norm:  0.2772980183362961\n",
      "********** epoch:  71 **********\n",
      "train l2_norm:  0.2615079832347957\n",
      "val l2_norm:  0.27710261444250744\n",
      "********** epoch:  72 **********\n",
      "train l2_norm:  0.25808932970870624\n",
      "val l2_norm:  0.2784328758716583\n",
      "********** epoch:  73 **********\n",
      "train l2_norm:  0.2568176036531275\n",
      "val l2_norm:  0.2770668814579646\n",
      "********** epoch:  74 **********\n",
      "train l2_norm:  0.25610786270011554\n",
      "val l2_norm:  0.2706729670365651\n",
      "********** epoch:  75 **********\n",
      "train l2_norm:  0.252044870772145\n",
      "val l2_norm:  0.27601950367291767\n",
      "********** epoch:  76 **********\n",
      "train l2_norm:  0.2527945109389045\n",
      "val l2_norm:  0.27679487069447833\n",
      "********** epoch:  77 **********\n",
      "train l2_norm:  0.2535569410432469\n",
      "val l2_norm:  0.2804465840260188\n",
      "********** epoch:  78 **********\n",
      "train l2_norm:  0.25180310823700647\n",
      "val l2_norm:  0.2828102906545003\n",
      "********** epoch:  79 **********\n",
      "train l2_norm:  0.2540194324471734\n",
      "val l2_norm:  0.30220725138982135\n",
      "********** epoch:  80 **********\n",
      "train l2_norm:  0.263614605773579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val l2_norm:  0.2716602683067322\n",
      "********** epoch:  81 **********\n",
      "train l2_norm:  0.2514918771657077\n",
      "val l2_norm:  0.2712254673242569\n",
      "********** epoch:  82 **********\n",
      "train l2_norm:  0.25100456313653424\n",
      "val l2_norm:  0.2759317606687546\n",
      "********** epoch:  83 **********\n",
      "train l2_norm:  0.247743735936555\n",
      "val l2_norm:  0.26873694856961566\n",
      "********** epoch:  84 **********\n",
      "train l2_norm:  0.24605465815825897\n",
      "val l2_norm:  0.27460040152072906\n",
      "********** epoch:  85 **********\n",
      "train l2_norm:  0.24485200304876675\n",
      "val l2_norm:  0.27199745178222656\n",
      "********** epoch:  86 **********\n",
      "train l2_norm:  0.24720364944501358\n",
      "val l2_norm:  0.2681963990132014\n",
      "********** epoch:  87 **********\n",
      "train l2_norm:  0.2409490475600416\n",
      "val l2_norm:  0.2744613438844681\n",
      "********** epoch:  88 **********\n",
      "train l2_norm:  0.24040075120600787\n",
      "val l2_norm:  0.2746249834696452\n",
      "********** epoch:  89 **********\n",
      "train l2_norm:  0.24212362481789154\n",
      "val l2_norm:  0.2777971824010213\n",
      "********** epoch:  90 **********\n",
      "train l2_norm:  0.2417826930230314\n",
      "val l2_norm:  0.2714252918958664\n",
      "********** epoch:  91 **********\n",
      "train l2_norm:  0.2394699278202924\n",
      "val l2_norm:  0.28085926671822864\n",
      "********** epoch:  92 **********\n",
      "train l2_norm:  0.24081132831898602\n",
      "val l2_norm:  0.2753909230232239\n",
      "********** epoch:  93 **********\n",
      "train l2_norm:  0.2377642819827253\n",
      "val l2_norm:  0.2688834220170975\n",
      "********** epoch:  94 **********\n",
      "train l2_norm:  0.23205786333842712\n",
      "val l2_norm:  0.2680762658516566\n",
      "********** epoch:  95 **********\n",
      "train l2_norm:  0.23075243627483194\n",
      "val l2_norm:  0.2661871959765752\n",
      "********** epoch:  96 **********\n",
      "train l2_norm:  0.22810861603780228\n",
      "val l2_norm:  0.2668361763159434\n",
      "********** epoch:  97 **********\n",
      "train l2_norm:  0.22775457189841705\n",
      "val l2_norm:  0.2654370218515396\n",
      "********** epoch:  98 **********\n",
      "train l2_norm:  0.22514361143112183\n",
      "val l2_norm:  0.26604433357715607\n",
      "********** epoch:  99 **********\n",
      "train l2_norm:  0.22344972735101526\n",
      "val l2_norm:  0.2675979286432266\n",
      "Minimum Valid Loss:  0.2654370218515396\n"
     ]
    }
   ],
   "source": [
    "val_loss = []\n",
    "for epoch in range(100):\n",
    "    print('*'*10, 'epoch: ', epoch, '*'*10)\n",
    "    for phase in ['train', 'valid']:\n",
    "        if phase == 'train':\n",
    "            loss_list = []\n",
    "            model.train()\n",
    "            for i, data in enumerate(data_loaders[phase]):\n",
    "                input, label, depth = data\n",
    "                input = input.to(device)\n",
    "                label = label.to(device)\n",
    "                depth = depth.to(device)\n",
    "                output = model(input)\n",
    "                loss = bce(output, label)\n",
    "                loss_list.append(loss.item())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            mean_loss = sum(loss_list) / len(loss_list)\n",
    "            print(\"train l2_norm: \", mean_loss)\n",
    "        elif phase == 'valid':\n",
    "            loss_list = []\n",
    "            model.eval()\n",
    "            for i, data in enumerate(data_loaders[phase]):\n",
    "                input, label, depth = data\n",
    "                input = input.to(device)\n",
    "                label = label.to(device)\n",
    "                depth = depth.to(device)\n",
    "                output = model(input)\n",
    "                loss = bce(output, label)\n",
    "                loss_list.append(loss.item())\n",
    "            mean_loss = sum(loss_list) / len(loss_list)\n",
    "            print(\"val l2_norm: \", mean_loss)\n",
    "            val_loss.append(mean_loss)\n",
    "print('Minimum Valid Loss: ', min(val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.1912, -4.5919, -3.7826,  1.0920, -0.1028, -1.4654, -2.3256, -3.2009,\n",
       "        -4.2802, -4.0113, -3.9642, -3.2082,  0.1989,  2.2389,  2.1062,  1.6563,\n",
       "         0.2250, -0.8211, -1.2382, -2.4558, -4.6144, -5.4535, -5.5181, -3.5093,\n",
       "        -2.0019, -1.3418, -2.5365, -2.9140, -2.0813, -1.3320, -1.4052, -2.4131,\n",
       "        -3.5022, -2.8820,  0.5741,  2.0301,  1.7948,  1.2874,  0.3553, -1.2804,\n",
       "        -2.7809, -2.4360, -2.3657, -1.7765, -1.2545, -0.8575, -0.4465, -1.9243,\n",
       "        -2.0214, -2.1075, -3.9384, -3.6413, -2.1267, -0.2893,  1.0302, -2.5001,\n",
       "        -3.1575, -1.3231, -1.5140, -3.5904, -3.4637, -2.5914, -1.2374, -0.3247,\n",
       "         1.3071,  2.4920,  3.0284,  2.8223,  1.7504,  0.4081, -0.1211, -0.2686,\n",
       "         0.1369,  0.7091,  1.1211,  1.0019,  0.4554,  0.4324, -0.5490, -1.9090,\n",
       "        -3.2285, -3.5028, -3.8826, -3.1641, -1.8152,  0.3677,  1.6592,  1.9254,\n",
       "         1.8045,  1.7680,  1.9620,  1.9949,  2.1259,  2.4794,  3.0494,  1.7757,\n",
       "         0.7657, -0.2240, -1.1123, -1.8806, -2.9614, -2.9063, -1.6876,  1.2611,\n",
       "        -0.6033, -0.7408, -0.4210, -0.1930,  0.0119, -0.3955, -1.2720, -0.5711,\n",
       "        -2.5110, -5.4216, -5.5710, -3.5800,  0.0815,  0.3500,  0.7587,  0.9145,\n",
       "         0.1537, -1.2250, -2.2280, -2.3756, -1.3806, -0.6136, -3.2245, -3.0090],\n",
       "       device='cuda:1', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=128x128 at 0x7FFF751C4390>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_pil((output[0][0].cpu()>0).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
