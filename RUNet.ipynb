{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_output', 'images_val', 'labels_val', 'test', 'labels', 'images']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import scipy.ndimage.morphology as morph\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "to_pil = transforms.ToPILImage()\n",
    "os.listdir('../r_unet/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Parameters\n",
    "'''\n",
    "\"\"\"\n",
    "cell_model = {\n",
    "    'Rnn' : ConvRnnCell(in_channels, out_channels), \n",
    "    'Gru' : ConvGruCell(in_channels, out_channels), \n",
    "    'Rrn' : ConvRrnCell(in_channels, out_channels), \n",
    "    'Sru' : ConvSruCell(in_channels, out_channels), \n",
    "    'Dru' : ConvDruCell(in_channels, out_channels)\n",
    "}\n",
    "\"\"\"\n",
    "RECURRENT = True\n",
    "PARAMETERS = {\n",
    "    'd1':False, \n",
    "    'd2':False, \n",
    "    'd3':False, \n",
    "    'b_':True, \n",
    "    'u1':False, \n",
    "    'u2':False, \n",
    "    'u3':False, \n",
    "    'cell_model':'Sru'\n",
    "}\n",
    "\n",
    "# arguments\n",
    "TIMESTEPS = 3\n",
    "BATCH_SIZE = 1\n",
    "NUM_EPOCHS = 200\n",
    "INPUT_SIZE = 128\n",
    "INPUT_CHANNELS = 1\n",
    "NUM_CLASSES = 2\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# decive\n",
    "DEVICE = \"cuda:1\"\n",
    "device = torch.device(DEVICE if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# way to the data folders\n",
    "FOLDER_DATA = \"../r_unet/data/images\"\n",
    "FOLDER_MASK = \"../r_unet/data/labels\"\n",
    "FOLDER_TEST = \"../r_unet/data/test\"\n",
    "FOLDER_DATA_VAL = \"../r_unet/data/images_val\"\n",
    "FOLDER_MASK_VAL = \"../r_unet/data/labels_val\"\n",
    "\n",
    "FILE_NAMES = sorted(os.listdir('../r_unet/data/images'))\n",
    "FILE_NAMES_VAL = sorted(os.listdir('../r_unet/data/images_val'))\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                              transforms.Resize((INPUT_SIZE, INPUT_SIZE), interpolation = 0),\n",
    "                              transforms.ToTensor()\n",
    "                              ])\n",
    "\n",
    "to_tensor = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dataloader\n",
    "'''\n",
    "def get_labels(object):\n",
    "    label1 = (object==0).float()\n",
    "    depth1 = to_tensor(morph.distance_transform_edt(np.asarray(label1[0])))\n",
    "    label2 = (label1==0).float()\n",
    "    depth2 = to_tensor(morph.distance_transform_edt(np.asarray(label2[0])))\n",
    "    labels = torch.stack([label1, label2], dim=1).squeeze()\n",
    "    depths = torch.stack([depth1, depth2], dim=1).squeeze()\n",
    "    return labels, depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainMedData(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.time = TIMESTEPS\n",
    "        self.folder_data = FOLDER_DATA\n",
    "        self.folder_mask = FOLDER_MASK\n",
    "        self.file_names = FILE_NAMES\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        gif_list = []\n",
    "        gif_list_depth = []\n",
    "        for i in range(self.time):\n",
    "            gif_list.append(transform(Image.open(self.folder_data + '/' + self.file_names[idx+i])))\n",
    "        gif_data = torch.stack(gif_list)\n",
    "        gif_list.clear()\n",
    "        for i in range(self.time):\n",
    "            label, depth = get_labels(transform(Image.open(self.folder_mask + '/' + self.file_names[idx+i])))\n",
    "            gif_list.append(label)\n",
    "            gif_list_depth.append(depth)\n",
    "        gif_mask = torch.stack(gif_list)\n",
    "        gif_depth = torch.stack(gif_list_depth)\n",
    "        gif_list.clear()\n",
    "        gif_list_depth.clear()\n",
    "        return gif_data, gif_mask, gif_depth\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_names) - self.time + 1\n",
    "\n",
    "\n",
    "class ValMedData(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.time = TIMESTEPS\n",
    "        self.folder_data = FOLDER_DATA_VAL\n",
    "        self.folder_mask = FOLDER_MASK_VAL\n",
    "        self.file_names = FILE_NAMES_VAL\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        gif_list = []\n",
    "        gif_list_depth = []\n",
    "        for i in range(self.time):\n",
    "            gif_list.append(transform(Image.open(self.folder_data + '/' + self.file_names[idx+i])))\n",
    "        gif_data = torch.stack(gif_list)\n",
    "        gif_list.clear()\n",
    "        for i in range(self.time):\n",
    "            label, depth = get_labels(transform(Image.open(self.folder_mask + '/' + self.file_names[idx+i])))\n",
    "            gif_list.append(label)\n",
    "            gif_list_depth.append(depth)\n",
    "        gif_mask = torch.stack(gif_list)\n",
    "        gif_depth = torch.stack(gif_list_depth)\n",
    "        gif_list.clear()\n",
    "        gif_list_depth.clear()\n",
    "        return gif_data, gif_mask, gif_depth\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names) - self.time + 1\n",
    "\n",
    "\n",
    "class TestMedData(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__\n",
    "        self.time = TIMESTEPS\n",
    "        self.folder_test = FOLDER_TEST\n",
    "        self.file_names = FILE_NAMES + FILE_NAMES_VAL\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        gif_list = []\n",
    "        for i in range(self.time):\n",
    "            gif_list.append(transform(Image.open(self.folder_test + '/' + self.file_names[idx+i])))\n",
    "        gif_test = torch.stack(gif_list)\n",
    "        gif_list.clear()\n",
    "        return gif_test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names) - self.time + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrainMedData()\n",
    "valid_dataset = ValMedData()\n",
    "test_dataset = TestMedData()\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          num_workers=1,\n",
    "                          shuffle=False)\n",
    "\n",
    "valid_loader = DataLoader(dataset=valid_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          num_workers=1,\n",
    "                          shuffle=False)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=1,\n",
    "                         num_workers=1,\n",
    "                         shuffle=False)\n",
    "\n",
    "data_loaders = {\n",
    "    'train' : train_loader,\n",
    "    'valid' : valid_loader,\n",
    "    'test' : test_loader\n",
    "}\n",
    "\n",
    "dataset_sizes = {\n",
    "    'train': len(train_dataset),\n",
    "    'valid': len(valid_dataset),\n",
    "    'test': len(test_dataset)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Recurrent cell\n",
    "'''\n",
    "class ConvRnnCell(nn.Module):    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvRnnCell, self).__init__()\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(in_channels+out_channels, out_channels, kernel_size=3, padding=1))\n",
    "             \n",
    "    def forward(self, x, hidden):\n",
    "        out = torch.cat([x, hidden],dim=1)\n",
    "        out = self.conv1(out)\n",
    "        hidden = out\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvGruCell(nn.Module):    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvGruCell, self).__init__()\n",
    "        self.conv_for_input = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        \n",
    "        self.conv_for_hidden = nn.Sequential(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))\n",
    "        \n",
    "        self.conv_2x_update = nn.Sequential(nn.Conv2d(in_channels+out_channels, out_channels, kernel_size=3, padding=1))\n",
    "        self.conv_2x_reset = nn.Sequential(nn.Conv2d(in_channels+out_channels, out_channels, kernel_size=3, padding=1))\n",
    "        \n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "     \n",
    "    def forward(self, x, hidden):\n",
    "        input = torch.cat([x, hidden],dim=1)\n",
    "\n",
    "        update_gate = self.conv_2x_update(input)\n",
    "        update_gate = self.sig((update_gate)) ### output after update gate\n",
    "        reset_gate = self.conv_2x_reset(input)\n",
    "        reset_gate = self.sig((reset_gate)) ### output after reset gate\n",
    "        \n",
    "        \n",
    "        memory_for_input = self.conv_for_input(x)\n",
    "        memory_for_hidden = self.conv_for_hidden(hidden)# просто хидден\n",
    "\n",
    "        memory_content = memory_for_input + (reset_gate * memory_for_hidden) ### output for reset gate(affects how the reset gate do work)\n",
    "        memory_content = self.relu(memory_content)\n",
    "\n",
    "        hidden = (update_gate * hidden) + ((1 - update_gate) * memory_content) # torch.ones(input_size, hidden_size)\n",
    "\n",
    "        return hidden, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvRrnCell(nn.Module):    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvRrnCell, self).__init__()\n",
    "        self.conv_for_input = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        \n",
    "        self.conv_for_hidden = nn.Sequential(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))\n",
    "        \n",
    "        self.conv_2x_update = nn.Sequential(nn.Conv2d(in_channels+out_channels, out_channels, kernel_size=3, padding=1))\n",
    "        self.conv_2x_reset = nn.Sequential(nn.Conv2d(in_channels+out_channels, out_channels, kernel_size=3, padding=1))\n",
    "        \n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "     \n",
    "    def forward(self, x, hidden):\n",
    "        input = torch.cat([x, hidden],dim=1)\n",
    "\n",
    "        update_gate = self.conv_2x_update(input)\n",
    "        update_gate = self.sig((update_gate)) \n",
    "        \n",
    "        memory_for_input = self.conv_for_input(x)\n",
    "        memory_for_hidden = hidden\n",
    "\n",
    "        memory_content = memory_for_input + memory_for_hidden\n",
    "        memory_content = self.relu(memory_content)\n",
    "\n",
    "        hidden = (update_gate * hidden) + ((1 - update_gate) * memory_content) # torch.ones(input_size, hidden_size)\n",
    "\n",
    "        return hidden, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvSruCell(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvSruCell, self).__init__()\n",
    "        self.update_gate = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.backbone = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        z = self.update_gate(x)\n",
    "        z = self.sig(z)\n",
    "        \n",
    "        h_ = self.backbone(x)\n",
    "        h_ = self.tanh(h_)\n",
    "        \n",
    "        h_prev = hidden * z\n",
    "        h = (1 - z) * h_\n",
    "        out = h + h_prev\n",
    "        return out, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDruCell(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvDruCell, self).__init__()\n",
    "        self.update_gate = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.reset_gate = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n",
    "        self.backbone = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        z = self.update_gate(x)\n",
    "        z = self.sig(z)\n",
    "        \n",
    "        r = self.reset_gate(x)\n",
    "        r = self.sig(r)\n",
    "        rx = r * x\n",
    "        h_ = self.backbone(rx)\n",
    "        h_ = self.tanh(h_)\n",
    "        \n",
    "        h_prev = hidden * z\n",
    "        h = (1 - z) * h_\n",
    "        out = h + h_prev\n",
    "        return out, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Model paths\n",
    "'''\n",
    "class ConvRnn(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, ConvRnn_input_size, cell_model, reccurent=RECURRENT): # arg for ConvRnn layer\n",
    "        super(ConvRnn, self).__init__()\n",
    "        self.cell_dict = {\n",
    "            'Rnn' : ConvRnnCell(in_channels, out_channels), \n",
    "            'Gru' : ConvGruCell(in_channels, out_channels), \n",
    "            'Rrn' : ConvRrnCell(in_channels, out_channels), \n",
    "            'Sru' : ConvSruCell(in_channels, out_channels), \n",
    "            'Dru' : ConvDruCell(in_channels, out_channels)\n",
    "        }\n",
    "        self.rec = reccurent\n",
    "        self.cell_model = cell_model\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.timesteps = TIMESTEPS\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.input_size = ConvRnn_input_size\n",
    "        self.hidden_size = (self.batch_size, self.out_channels, self.input_size, self.input_size)\n",
    "        \n",
    "        self.ConvRnn_layer = self.cell_dict[self.cell_model]\n",
    "        self.init_hidden = torch.zeros(self.hidden_size).to(device)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_cells = None\n",
    "        x_list = []\n",
    "\n",
    "        x = x.reshape(self.batch_size, self.timesteps, self.in_channels, self.input_size, self.input_size)\n",
    "        x = x.permute(1, 0, 2, 3, 4)\n",
    "        if self.rec == True:\n",
    "            for i in range(self.timesteps):\n",
    "                if x_cells is None:\n",
    "                    x_cells, hidden = self.ConvRnn_layer(x[i], self.init_hidden)\n",
    "                    x_list.append(x_cells)\n",
    "                else:\n",
    "                    x_i, hidden = self.ConvRnn_layer(x[i], hidden)\n",
    "                    x_list.append(x_i)\n",
    "        elif self.rec == False:\n",
    "            for i in range(self.timesteps):\n",
    "                if x_cells is None:\n",
    "                    x_cells, _ = self.ConvRnn_layer(x[i], self.init_hidden)\n",
    "                    x_list.append(x_cells)\n",
    "                else:\n",
    "                    x_i, _ = self.ConvRnn_layer(x[i], self.init_hidden)\n",
    "                    x_list.append(x_i)\n",
    "        else:\n",
    "            print('RECURRENT can be only True or False')\n",
    "            quit()\n",
    "        x_cells = torch.stack(x_list)\n",
    "        x_cells = x_cells.permute(1, 0, 2, 3, 4)\n",
    "\n",
    "        x_cells = x_cells.reshape(-1, self.out_channels, self.input_size, self.input_size)\n",
    "        return x_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvRnnRelu(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels ,ConvRnn_input_size, cell_model):\n",
    "        super(ConvRnnRelu, self).__init__()\n",
    "        self.cell_model = cell_model\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.input_size = ConvRnn_input_size\n",
    "        self.convrnnrelu = nn.Sequential(ConvRnn(self.in_channels, self.out_channels, \n",
    "                                                 self.input_size, self.cell_model),\n",
    "                                         nn.ReLU()\n",
    "                                         )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.convrnnrelu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvRelu(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvRelu, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.convrelu = nn.Sequential(nn.Conv2d(self.in_channels, self.out_channels, kernel_size=3, padding=1),\n",
    "                                      nn.ReLU()\n",
    "                                      )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.convrelu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaxPool, self).__init__()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.maxpool(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpAndCat(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(UpAndCat, self).__init__()\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "\n",
    "    def forward(self, x_up, x_cat):\n",
    "        out = self.up(x_up)\n",
    "        out = torch.cat([out, x_cat], dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Model head\n",
    "'''\n",
    "class UNetDesigner(nn.Module):    \n",
    "    def __init__(self, d1, d2, d3, b_, u1, u2, u3, cell_model,\n",
    "                 input_size=INPUT_SIZE, input_channels=INPUT_CHANNELS, num_classes=NUM_CLASSES):\n",
    "        super(UNetDesigner, self).__init__()\n",
    "        self.cell_model = cell_model\n",
    "        self.num_classes = NUM_CLASSES\n",
    "        self.d1, self.d2, self.d3, self.b, self.u1, self.u2, self.u3 = d1, d2, d3, b_, u1, u2, u3\n",
    "        self.input_size = input_size\n",
    "        self.input_chennels = input_channels\n",
    "        self.ch_list = [self.input_chennels, 32, 64, 128, 256]\n",
    "        self.input_x2 = int(self.input_size / 2)\n",
    "        self.input_x4 = int(self.input_size / 4)\n",
    "        self.input_x8 = int(self.input_size / 8)\n",
    "\n",
    "         ##### Down_1 layer ##### input_size = 128\n",
    "        if self.d1 == True:\n",
    "            self.down1 = nn.Sequential(ConvRnnRelu(self.ch_list[0], self.ch_list[1],    # Channels\n",
    "                                                   self.input_size, self.cell_model),               \n",
    "                                       ConvRelu(self.ch_list[1], self.ch_list[1])                   \n",
    "                                       )                                                # 1  -->32\n",
    "        else:                                                                           # 32 -->32\n",
    "            self.down1 = nn.Sequential(ConvRelu(self.ch_list[0], self.ch_list[1]),\n",
    "                                       ConvRelu(self.ch_list[1], self.ch_list[1])\n",
    "                                       )\n",
    "        self.down1_pool = MaxPool()\n",
    "\n",
    "         ##### Down_2 layer ##### input_size = 64\n",
    "        if self.d2 == True:\n",
    "            self.down2 = nn.Sequential(ConvRnnRelu(self.ch_list[1], self.ch_list[2],\n",
    "                                                   self.input_x2, self.cell_model),\n",
    "                                       ConvRelu(self.ch_list[2], self.ch_list[2])\n",
    "                                       )                                                # 32 -->64\n",
    "        else:                                                                           # 64 -->64\n",
    "            self.down2 = nn.Sequential(ConvRelu(self.ch_list[1], self.ch_list[2]),\n",
    "                                       ConvRelu(self.ch_list[2], self.ch_list[2])\n",
    "                                       )\n",
    "        self.down2_pool = MaxPool()\n",
    "\n",
    "         ##### Down_3 layer ##### input_size = 32\n",
    "        if self.d3 == True:\n",
    "            self.down3 = nn.Sequential(ConvRnnRelu(self.ch_list[2], self.ch_list[3], \n",
    "                                                   self.input_x4, self.cell_model),\n",
    "                                       ConvRelu(self.ch_list[3], self.ch_list[3])\n",
    "                                       )                                                # 64 -->128\n",
    "        else:                                                                           # 128-->128\n",
    "            self.down3 = nn.Sequential(ConvRelu(self.ch_list[2], self.ch_list[3]),\n",
    "                                       ConvRelu(self.ch_list[3], self.ch_list[3])\n",
    "                                       )\n",
    "        self.down3_pool = MaxPool()\n",
    "\n",
    "         ##### Bottom layer ##### input_size = 16\n",
    "        if self.b == True:\n",
    "            self.bottom = nn.Sequential(ConvRnnRelu(self.ch_list[3], self.ch_list[4], \n",
    "                                                    self.input_x8, self.cell_model),\n",
    "                                        ConvRelu(self.ch_list[4], self.ch_list[4])\n",
    "                                        )                                               # 128-->256\n",
    "        else:                                                                           # 256-->256\n",
    "            self.bottom = nn.Sequential(ConvRelu(self.ch_list[3], self.ch_list[4]),\n",
    "                                        ConvRelu(self.ch_list[4], self.ch_list[4])\n",
    "                                        )\n",
    "\n",
    "         ##### Up_3 layer #####\n",
    "        self.up_cat_3 = UpAndCat()\n",
    "        if self.u3 == True:\n",
    "            self.up_conv_3 = nn.Sequential(ConvRnnRelu(self.ch_list[4]+self.ch_list[3], \n",
    "                                                       self.ch_list[3], \n",
    "                                                       self.input_x4, self.cell_model),\n",
    "                                           ConvRelu(self.ch_list[3], self.ch_list[3])\n",
    "                                           )                                            # 394-->128\n",
    "        else:                                                                           # 128-->128\n",
    "            self.up_conv_3 = nn.Sequential(ConvRelu(self.ch_list[4]+self.ch_list[3], \n",
    "                                                    self.ch_list[3]),\n",
    "                                           ConvRelu(self.ch_list[3], self.ch_list[3])\n",
    "                                           )\n",
    "\n",
    "         ##### Up_2 layer #####\n",
    "        self.up_cat_2 = UpAndCat()\n",
    "        if self.u2 == True:\n",
    "            self.up_conv_2 = nn.Sequential(ConvRnnRelu(self.ch_list[3]+self.ch_list[2], \n",
    "                                                       self.ch_list[2], \n",
    "                                                       self.input_x2, self.cell_model),\n",
    "                                           ConvRelu(self.ch_list[2], self.ch_list[2])\n",
    "                                           )                                            # 192-->64\n",
    "        else:                                                                           # 64 -->64\n",
    "            self.up_conv_2 = nn.Sequential(ConvRelu(self.ch_list[3]+self.ch_list[2], \n",
    "                                                    self.ch_list[2]),\n",
    "                                           ConvRelu(self.ch_list[2], self.ch_list[2])\n",
    "                                           )\n",
    "\n",
    "         ##### Up_1 layer #####\n",
    "        self.up_cat_1 = UpAndCat()\n",
    "        if self.u1 == True:\n",
    "            self.up_conv_1 = nn.Sequential(ConvRnnRelu(self.ch_list[2]+self.ch_list[1], \n",
    "                                                       self.ch_list[1], \n",
    "                                                       self.input_size, self.cell_model),\n",
    "                                           ConvRelu(self.ch_list[1], self.ch_list[1])\n",
    "                                           )                                            # 96 -->32\n",
    "        else:                                                                           # 32 -->32\n",
    "            self.up_conv_1 = nn.Sequential(ConvRelu(self.ch_list[2]+self.ch_list[1], \n",
    "                                                    self.ch_list[1]),\n",
    "                                           ConvRelu(self.ch_list[1], self.ch_list[1])\n",
    "                                           )\n",
    "\n",
    "         ##### Final layer #####\n",
    "        self.final = nn.Sequential(nn.Conv2d(self.ch_list[1], self.num_classes, kernel_size=1),\n",
    "\n",
    "                                   )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, self.input_chennels, self.input_size, self.input_size)\n",
    "        # print(x.shape)\n",
    "        down1_feat = self.down1(x)\n",
    "        pool1 = self.down1_pool(down1_feat)\n",
    "        # print(pool1.shape)\n",
    "        down2_feat = self.down2(pool1)\n",
    "        pool2 = self.down2_pool(down2_feat)\n",
    "        # print(pool2.shape)\n",
    "        down3_feat = self.down3(pool2)\n",
    "        pool3 = self.down3_pool(down3_feat)\n",
    "        # print(pool3.shape)\n",
    "        bottom_feat = self.bottom(pool3)\n",
    "        # print(bottom_feat.shape)\n",
    "        up_feat3 = self.up_cat_3(bottom_feat, down3_feat)\n",
    "        up_feat3 = self.up_conv_3(up_feat3)\n",
    "        \n",
    "        up_feat2 = self.up_cat_2(up_feat3, down2_feat)\n",
    "        up_feat2 = self.up_conv_2(up_feat2)\n",
    "        \n",
    "        up_feat1 = self.up_cat_1(up_feat2, down1_feat)\n",
    "        up_feat1 = self.up_conv_1(up_feat1)\n",
    "        \n",
    "        out = self.final(up_feat1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNetDesigner(d1=PARAMETERS['d1'], \n",
    "                     d2=PARAMETERS['d2'], \n",
    "                     d3=PARAMETERS['d3'], \n",
    "                     b_=PARAMETERS['b_'], \n",
    "                     u1=PARAMETERS['u1'], \n",
    "                     u2=PARAMETERS['u2'], \n",
    "                     u3=PARAMETERS['u3'], \n",
    "                     cell_model=PARAMETERS['cell_model']\n",
    "                     )\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Loss\n",
    "'''\n",
    "def l2_loss(x, y, d):\n",
    "    d = d.reshape(x.shape)\n",
    "    y = y.reshape(x.shape)\n",
    "    x = torch.sigmoid(x)\n",
    "    out = ((x - y*d**2)**2).sum()\n",
    "    return out\n",
    "\n",
    "def bce_loss(x, y, d):\n",
    "    y = y.reshape(x.shape)\n",
    "    return F.binary_cross_entropy_with_logits(x, y)\n",
    "\n",
    "def dice_loss(x, y, d):\n",
    "    d = d.reshape(x.shape)\n",
    "    y = y.reshape(x.shape)\n",
    "    x = torch.sigmoid(x)\n",
    "    intersection = (x * y*d**2).sum(dim=2).sum(dim=2)\n",
    "    x_sum = (x*d**2).sum(dim=2).sum(dim=2)\n",
    "    y_sum = (y*d**2).sum(dim=2).sum(dim=2)\n",
    "    dice_loss = 1 - (2*intersection / (x_sum + y_sum))\n",
    "    return dice_loss.mean()\n",
    "\n",
    "def combo_loss(x, y, d, bce_weight=0.5):\n",
    "    combo_loss = bce_weight * bce_loss(x, y, d) + (1 - bce_weight) * dice_loss(x, y, d)\n",
    "    return combo_loss\n",
    "\n",
    "def l2_combo_loss(x, y, d):\n",
    "    l2_combo_loss = l2_loss(x, y, d) * bce_loss(x, y, d)\n",
    "    return l2_combo_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Metric\n",
    "'''\n",
    "def IoU_metric(x, y):\n",
    "    y = y.reshape(x.shape)\n",
    "    x = torch.sigmoid(x)\n",
    "    intersection = (x * y).sum(dim=2).sum(dim=2)\n",
    "    x_sum = x.sum(dim=2).sum(dim=2)\n",
    "    y_sum = y.sum(dim=2).sum(dim=2)\n",
    "    IoU_metric = intersection / (x_sum + y_sum - intersection)\n",
    "    return IoU_metric.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** epoch:  0 **********\n",
      "train l2_norm:  0.3652277019890872\n",
      "val l2_norm:  0.3945416708787282\n",
      "********** epoch:  1 **********\n",
      "train l2_norm:  0.40692706270651385\n",
      "val l2_norm:  0.4457744111617406\n",
      "********** epoch:  2 **********\n",
      "train l2_norm:  0.46919121660969476\n",
      "val l2_norm:  0.44102800885836285\n",
      "********** epoch:  3 **********\n",
      "train l2_norm:  0.48546694760972803\n",
      "val l2_norm:  0.4888252665599187\n",
      "********** epoch:  4 **********\n",
      "train l2_norm:  0.5115755010734905\n",
      "val l2_norm:  0.5150277415911356\n",
      "********** epoch:  5 **********\n",
      "train l2_norm:  0.5360615849494934\n",
      "val l2_norm:  0.5162723163763682\n",
      "********** epoch:  6 **********\n",
      "train l2_norm:  0.5377317775379528\n",
      "val l2_norm:  0.5185018678506216\n",
      "********** epoch:  7 **********\n",
      "train l2_norm:  0.5405929711731997\n",
      "val l2_norm:  0.5209670762221018\n",
      "********** epoch:  8 **********\n",
      "train l2_norm:  0.5425978980281136\n",
      "val l2_norm:  0.5254094302654266\n",
      "********** epoch:  9 **********\n",
      "train l2_norm:  0.5463015139102936\n",
      "val l2_norm:  0.519290049870809\n",
      "********** epoch:  10 **********\n",
      "train l2_norm:  0.5448015359315005\n",
      "val l2_norm:  0.5366052885850271\n",
      "********** epoch:  11 **********\n",
      "train l2_norm:  0.5569746494293213\n",
      "val l2_norm:  0.5290294289588928\n",
      "********** epoch:  12 **********\n",
      "train l2_norm:  0.5531801256266508\n",
      "val l2_norm:  0.5426080226898193\n",
      "********** epoch:  13 **********\n",
      "train l2_norm:  0.5570110976696014\n",
      "val l2_norm:  0.5357384781042734\n",
      "********** epoch:  14 **********\n",
      "train l2_norm:  0.5654375200921838\n",
      "val l2_norm:  0.546916256348292\n",
      "********** epoch:  15 **********\n",
      "train l2_norm:  0.5668209249323065\n",
      "val l2_norm:  0.5472534199555715\n",
      "********** epoch:  16 **********\n",
      "train l2_norm:  0.568964421749115\n",
      "val l2_norm:  0.5410238405068716\n",
      "********** epoch:  17 **********\n",
      "train l2_norm:  0.5693686414848674\n",
      "val l2_norm:  0.5446520348389944\n",
      "********** epoch:  18 **********\n",
      "train l2_norm:  0.5698410489342429\n",
      "val l2_norm:  0.5433109800020853\n",
      "********** epoch:  19 **********\n",
      "train l2_norm:  0.5688317011703145\n",
      "val l2_norm:  0.5592219432195028\n",
      "********** epoch:  20 **********\n",
      "train l2_norm:  0.5787147771228444\n",
      "val l2_norm:  0.5569023887316386\n",
      "********** epoch:  21 **********\n",
      "train l2_norm:  0.5749270915985107\n",
      "val l2_norm:  0.55374609430631\n",
      "********** epoch:  22 **********\n",
      "train l2_norm:  0.5774340060624209\n",
      "val l2_norm:  0.5546230375766754\n",
      "********** epoch:  23 **********\n",
      "train l2_norm:  0.5847223699092865\n",
      "val l2_norm:  0.575191855430603\n",
      "********** epoch:  24 **********\n",
      "train l2_norm:  0.581693475896662\n",
      "val l2_norm:  0.5625017484029134\n",
      "********** epoch:  25 **********\n",
      "train l2_norm:  0.5864420397715135\n",
      "val l2_norm:  0.5649422208468119\n",
      "********** epoch:  26 **********\n",
      "train l2_norm:  0.5889693742448633\n",
      "val l2_norm:  0.5675347248713175\n",
      "********** epoch:  27 **********\n",
      "train l2_norm:  0.5851559611884031\n",
      "val l2_norm:  0.5633654892444611\n",
      "********** epoch:  28 **********\n",
      "train l2_norm:  0.5925137996673584\n",
      "val l2_norm:  0.5771979987621307\n",
      "********** epoch:  29 **********\n",
      "train l2_norm:  0.5913688350807537\n",
      "val l2_norm:  0.5770629247029623\n",
      "********** epoch:  30 **********\n",
      "train l2_norm:  0.6018127826127139\n",
      "val l2_norm:  0.5822275678316752\n",
      "********** epoch:  31 **********\n",
      "train l2_norm:  0.5983431989496405\n",
      "val l2_norm:  0.5896064837773641\n",
      "********** epoch:  32 **********\n",
      "train l2_norm:  0.6044064245440743\n",
      "val l2_norm:  0.5830202500025431\n",
      "********** epoch:  33 **********\n",
      "train l2_norm:  0.6049854050983082\n",
      "val l2_norm:  0.5859675407409668\n",
      "********** epoch:  34 **********\n",
      "train l2_norm:  0.6097757518291473\n",
      "val l2_norm:  0.6100846230983734\n",
      "********** epoch:  35 **********\n",
      "train l2_norm:  0.6133602884682742\n",
      "val l2_norm:  0.617499053478241\n",
      "********** epoch:  36 **********\n",
      "train l2_norm:  0.6210307316346602\n",
      "val l2_norm:  0.5993195871512095\n",
      "********** epoch:  37 **********\n",
      "train l2_norm:  0.6067045574838464\n",
      "val l2_norm:  0.578244020541509\n",
      "********** epoch:  38 **********\n",
      "train l2_norm:  0.6125246665694497\n",
      "val l2_norm:  0.6029066840807596\n",
      "********** epoch:  39 **********\n",
      "train l2_norm:  0.6215794519944624\n",
      "val l2_norm:  0.5911727050940195\n",
      "********** epoch:  40 **********\n",
      "train l2_norm:  0.6172076490792361\n",
      "val l2_norm:  0.5997769435246786\n",
      "********** epoch:  41 **********\n",
      "train l2_norm:  0.6275053051385012\n",
      "val l2_norm:  0.6028387248516083\n",
      "********** epoch:  42 **********\n",
      "train l2_norm:  0.6310544908046722\n",
      "val l2_norm:  0.6062736213207245\n",
      "********** epoch:  43 **********\n",
      "train l2_norm:  0.6357171454212882\n",
      "val l2_norm:  0.6138990422089895\n",
      "********** epoch:  44 **********\n",
      "train l2_norm:  0.6405664384365082\n",
      "val l2_norm:  0.6182632744312286\n",
      "********** epoch:  45 **********\n",
      "train l2_norm:  0.6436907242644917\n",
      "val l2_norm:  0.6176320811112722\n",
      "********** epoch:  46 **********\n",
      "train l2_norm:  0.6420695104382255\n",
      "val l2_norm:  0.6107065280278524\n",
      "********** epoch:  47 **********\n",
      "train l2_norm:  0.6478008411147378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _releaseLock at 0x7ffff6b9e9e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/headless/anaconda3/lib/python3.7/logging/__init__.py\", line 221, in _releaseLock\n",
      "    def _releaseLock():\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val l2_norm:  0.6199631293614706\n",
      "********** epoch:  48 **********\n",
      "train l2_norm:  0.6457224677909504\n",
      "val l2_norm:  0.6205129822095236\n",
      "********** epoch:  49 **********\n",
      "train l2_norm:  0.6523416882211511\n",
      "val l2_norm:  0.6142878035704294\n",
      "********** epoch:  50 **********\n",
      "train l2_norm:  0.6558084515008059\n",
      "val l2_norm:  0.6308067540327708\n",
      "********** epoch:  51 **********\n",
      "train l2_norm:  0.6651818264614452\n",
      "val l2_norm:  0.6292847494284312\n",
      "********** epoch:  52 **********\n",
      "train l2_norm:  0.6663625592535193\n",
      "val l2_norm:  0.6301521062850952\n",
      "********** epoch:  53 **********\n",
      "train l2_norm:  0.670296622948213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _releaseLock at 0x7ffff6b9e9e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/headless/anaconda3/lib/python3.7/logging/__init__.py\", line 221, in _releaseLock\n",
      "    def _releaseLock():\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val l2_norm:  0.6314391295115153\n",
      "********** epoch:  54 **********\n",
      "train l2_norm:  0.6727969185872511\n",
      "val l2_norm:  0.6367430686950684\n",
      "********** epoch:  55 **********\n",
      "train l2_norm:  0.6769125813787634\n",
      "val l2_norm:  0.6403626104195913\n",
      "********** epoch:  56 **********\n",
      "train l2_norm:  0.6815514212304895\n",
      "val l2_norm:  0.6492381493250529\n",
      "********** epoch:  57 **********\n",
      "train l2_norm:  0.6862533824010328\n",
      "val l2_norm:  0.6540986100832621\n",
      "********** epoch:  58 **********\n",
      "train l2_norm:  0.6919103048064492\n",
      "val l2_norm:  0.6564304133256277\n",
      "********** epoch:  59 **********\n",
      "train l2_norm:  0.6954361986030232\n",
      "val l2_norm:  0.6574970384438833\n",
      "********** epoch:  60 **********\n",
      "train l2_norm:  0.6992687610062686\n",
      "val l2_norm:  0.6633946100870768\n",
      "********** epoch:  61 **********\n",
      "train l2_norm:  0.7025882655924017\n",
      "val l2_norm:  0.6667546927928925\n",
      "********** epoch:  62 **********\n",
      "train l2_norm:  0.703276748006994\n",
      "val l2_norm:  0.6638682583967844\n",
      "********** epoch:  63 **********\n",
      "train l2_norm:  0.7107534273104235\n",
      "val l2_norm:  0.6699415147304535\n",
      "********** epoch:  64 **********\n",
      "train l2_norm:  0.71653735908595\n",
      "val l2_norm:  0.6656039555867513\n",
      "********** epoch:  65 **********\n",
      "train l2_norm:  0.7221086323261261\n",
      "val l2_norm:  0.6697340408960978\n",
      "********** epoch:  66 **********\n",
      "train l2_norm:  0.7275616038929332\n",
      "val l2_norm:  0.6741248865922292\n",
      "********** epoch:  67 **********\n",
      "train l2_norm:  0.7304351004687223\n",
      "val l2_norm:  0.6729025145371755\n",
      "********** epoch:  68 **********\n",
      "train l2_norm:  0.7326103882356123\n",
      "val l2_norm:  0.6714288095633189\n",
      "********** epoch:  69 **********\n",
      "train l2_norm:  0.7349449829621748\n",
      "val l2_norm:  0.6721671720345815\n",
      "********** epoch:  70 **********\n",
      "train l2_norm:  0.7370830313725905\n",
      "val l2_norm:  0.6736601193745931\n",
      "********** epoch:  71 **********\n",
      "train l2_norm:  0.7424246126955206\n",
      "val l2_norm:  0.6748170157273611\n",
      "********** epoch:  72 **********\n",
      "train l2_norm:  0.7457038949836384\n",
      "val l2_norm:  0.6774672369162241\n",
      "********** epoch:  73 **********\n",
      "train l2_norm:  0.7490542856129733\n",
      "val l2_norm:  0.6778609653313955\n",
      "********** epoch:  74 **********\n",
      "train l2_norm:  0.7510413527488708\n",
      "val l2_norm:  0.6797549525896708\n",
      "********** epoch:  75 **********\n",
      "train l2_norm:  0.7489463172175668\n",
      "val l2_norm:  0.6780918737252554\n",
      "********** epoch:  76 **********\n",
      "train l2_norm:  0.7530475231734189\n",
      "val l2_norm:  0.6790274182955424\n",
      "********** epoch:  77 **********\n",
      "train l2_norm:  0.7601214024153623\n",
      "val l2_norm:  0.6784582237402598\n",
      "********** epoch:  78 **********\n",
      "train l2_norm:  0.7650460844690149\n",
      "val l2_norm:  0.6838006774584452\n",
      "********** epoch:  79 **********\n",
      "train l2_norm:  0.7675347219813954\n",
      "val l2_norm:  0.6802504261334738\n",
      "********** epoch:  80 **********\n",
      "train l2_norm:  0.7673901075666602\n",
      "val l2_norm:  0.681964268287023\n",
      "********** epoch:  81 **********\n",
      "train l2_norm:  0.768510715527968\n",
      "val l2_norm:  0.6864167551199595\n",
      "********** epoch:  82 **********\n",
      "train l2_norm:  0.7689788802103563\n",
      "val l2_norm:  0.6898279786109924\n",
      "********** epoch:  83 **********\n",
      "train l2_norm:  0.7729805897582661\n",
      "val l2_norm:  0.6896662811438242\n",
      "********** epoch:  84 **********\n",
      "train l2_norm:  0.7807831195267764\n",
      "val l2_norm:  0.6864134669303894\n",
      "********** epoch:  85 **********\n",
      "train l2_norm:  0.7864011363549666\n",
      "val l2_norm:  0.6902808845043182\n",
      "********** epoch:  86 **********\n",
      "train l2_norm:  0.785830018195239\n",
      "val l2_norm:  0.6847538749376932\n",
      "********** epoch:  87 **********\n",
      "train l2_norm:  0.7861873642964796\n",
      "val l2_norm:  0.6807373960812887\n",
      "********** epoch:  88 **********\n",
      "train l2_norm:  0.7866954126141288\n",
      "val l2_norm:  0.6840167542298635\n",
      "********** epoch:  89 **********\n",
      "train l2_norm:  0.7848337184299122\n",
      "val l2_norm:  0.6873247524102529\n",
      "********** epoch:  90 **********\n",
      "train l2_norm:  0.7845394638451663\n",
      "val l2_norm:  0.6873670319716135\n",
      "********** epoch:  91 **********\n",
      "train l2_norm:  0.7903648479418321\n",
      "val l2_norm:  0.6934282879034678\n",
      "********** epoch:  92 **********\n",
      "train l2_norm:  0.7932684611190449\n",
      "val l2_norm:  0.6941133042176565\n",
      "********** epoch:  93 **********\n",
      "train l2_norm:  0.7999682480638678\n",
      "val l2_norm:  0.6902643541495005\n",
      "********** epoch:  94 **********\n",
      "train l2_norm:  0.8052067702466791\n",
      "val l2_norm:  0.6908730467160543\n",
      "********** epoch:  95 **********\n",
      "train l2_norm:  0.8093262179331346\n",
      "val l2_norm:  0.6932701667149862\n",
      "********** epoch:  96 **********\n",
      "train l2_norm:  0.8039753139019012\n",
      "val l2_norm:  0.692848136027654\n",
      "********** epoch:  97 **********\n",
      "train l2_norm:  0.8000745421106165\n",
      "val l2_norm:  0.6859572927157084\n",
      "********** epoch:  98 **********\n",
      "train l2_norm:  0.7993782162666321\n",
      "val l2_norm:  0.6868616541226705\n",
      "********** epoch:  99 **********\n",
      "train l2_norm:  0.8044354807246815\n",
      "val l2_norm:  0.6914159655570984\n",
      "********** epoch:  100 **********\n",
      "train l2_norm:  0.8126914528283206\n",
      "val l2_norm:  0.6888859967390696\n",
      "********** epoch:  101 **********\n",
      "train l2_norm:  0.8213666406544772\n",
      "val l2_norm:  0.6888937552769979\n",
      "********** epoch:  102 **********\n",
      "train l2_norm:  0.8241892484101382\n",
      "val l2_norm:  0.6924336850643158\n",
      "********** epoch:  103 **********\n",
      "train l2_norm:  0.822673887014389\n",
      "val l2_norm:  0.6943856577078501\n",
      "********** epoch:  104 **********\n",
      "train l2_norm:  0.8166736770759929\n",
      "val l2_norm:  0.6866313517093658\n",
      "********** epoch:  105 **********\n",
      "train l2_norm:  0.8069776025685397\n",
      "val l2_norm:  0.6871908803780874\n",
      "********** epoch:  106 **********\n",
      "train l2_norm:  0.7947566969828173\n",
      "val l2_norm:  0.690182109673818\n",
      "********** epoch:  107 **********\n",
      "train l2_norm:  0.8053842105648734\n",
      "val l2_norm:  0.6926964918772379\n",
      "********** epoch:  108 **********\n",
      "train l2_norm:  0.8076692033897747\n",
      "val l2_norm:  0.6956556737422943\n",
      "********** epoch:  109 **********\n",
      "train l2_norm:  0.812307905067097\n",
      "val l2_norm:  0.7012034555276235\n",
      "********** epoch:  110 **********\n",
      "train l2_norm:  0.8280377631837671\n",
      "val l2_norm:  0.7019648253917694\n",
      "********** epoch:  111 **********\n",
      "train l2_norm:  0.8331021260131489\n",
      "val l2_norm:  0.7019273142019907\n",
      "********** epoch:  112 **********\n",
      "train l2_norm:  0.834889986298301\n",
      "val l2_norm:  0.6986949841181437\n",
      "********** epoch:  113 **********\n",
      "train l2_norm:  0.8347788127985868\n",
      "val l2_norm:  0.6958024799823761\n",
      "********** epoch:  114 **********\n",
      "train l2_norm:  0.8327395130287517\n",
      "val l2_norm:  0.6920553743839264\n",
      "********** epoch:  115 **********\n",
      "train l2_norm:  0.8331055505709215\n",
      "val l2_norm:  0.692888488372167\n",
      "********** epoch:  116 **********\n",
      "train l2_norm:  0.8338573304089633\n",
      "val l2_norm:  0.6987294455369314\n",
      "********** epoch:  117 **********\n",
      "train l2_norm:  0.8400777280330658\n",
      "val l2_norm:  0.7042728761831919\n",
      "********** epoch:  118 **********\n",
      "train l2_norm:  0.8495943979783491\n",
      "val l2_norm:  0.7056567668914795\n",
      "********** epoch:  119 **********\n",
      "train l2_norm:  0.8570064089514993\n",
      "val l2_norm:  0.7078430652618408\n",
      "********** epoch:  120 **********\n",
      "train l2_norm:  0.8603230985728177\n",
      "val l2_norm:  0.7068560322125753\n",
      "********** epoch:  121 **********\n",
      "train l2_norm:  0.8617269207130779\n",
      "val l2_norm:  0.7046529650688171\n",
      "********** epoch:  122 **********\n",
      "train l2_norm:  0.8586667721921747\n",
      "val l2_norm:  0.7026179432868958\n",
      "********** epoch:  123 **********\n",
      "train l2_norm:  0.8578464361754331\n",
      "val l2_norm:  0.6985438863436381\n",
      "********** epoch:  124 **********\n",
      "train l2_norm:  0.8579187393188477\n",
      "val l2_norm:  0.6960229277610779\n",
      "********** epoch:  125 **********\n",
      "train l2_norm:  0.8565410321409052\n",
      "val l2_norm:  0.6926311651865641\n",
      "********** epoch:  126 **********\n",
      "train l2_norm:  0.8569809279658578\n",
      "val l2_norm:  0.6949310998121897\n",
      "********** epoch:  127 **********\n",
      "train l2_norm:  0.8607991310683164\n",
      "val l2_norm:  0.6962270239988962\n",
      "********** epoch:  128 **********\n",
      "train l2_norm:  0.8638614253564314\n",
      "val l2_norm:  0.6914170583089193\n",
      "********** epoch:  129 **********\n",
      "train l2_norm:  0.8626370240341533\n",
      "val l2_norm:  0.6871923406918844\n",
      "********** epoch:  130 **********\n",
      "train l2_norm:  0.8621663830497048\n",
      "val l2_norm:  0.6916483839352926\n",
      "********** epoch:  131 **********\n",
      "train l2_norm:  0.8657760755582289\n",
      "val l2_norm:  0.6939457754294077\n",
      "********** epoch:  132 **********\n",
      "train l2_norm:  0.8653142939914357\n",
      "val l2_norm:  0.6925195157527924\n",
      "********** epoch:  133 **********\n",
      "train l2_norm:  0.8634869727221403\n",
      "val l2_norm:  0.6945276260375977\n",
      "********** epoch:  134 **********\n",
      "train l2_norm:  0.8632247908548876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val l2_norm:  0.7020853161811829\n",
      "********** epoch:  135 **********\n",
      "train l2_norm:  0.862613938071511\n",
      "val l2_norm:  0.6999084750811259\n",
      "********** epoch:  136 **********\n",
      "train l2_norm:  0.8636598316105929\n",
      "val l2_norm:  0.6903146704037985\n",
      "********** epoch:  137 **********\n",
      "train l2_norm:  0.8656389523636211\n",
      "val l2_norm:  0.6927795012791952\n",
      "********** epoch:  138 **********\n",
      "train l2_norm:  0.8658562519333579\n",
      "val l2_norm:  0.6921724478403727\n",
      "********** epoch:  139 **********\n",
      "train l2_norm:  0.8705529028719122\n",
      "val l2_norm:  0.6992583076159159\n",
      "********** epoch:  140 **********\n",
      "train l2_norm:  0.8748581870035692\n",
      "val l2_norm:  0.7006691594918569\n",
      "********** epoch:  141 **********\n",
      "train l2_norm:  0.8755485740574923\n",
      "val l2_norm:  0.6957240998744965\n",
      "********** epoch:  142 **********\n",
      "train l2_norm:  0.8737253059040416\n",
      "val l2_norm:  0.7026797433694204\n",
      "********** epoch:  143 **********\n",
      "train l2_norm:  0.8748844563961029\n",
      "val l2_norm:  0.7027569611867269\n",
      "********** epoch:  144 **********\n",
      "train l2_norm:  0.8814752670851621\n",
      "val l2_norm:  0.700268824895223\n",
      "********** epoch:  145 **********\n",
      "train l2_norm:  0.8922076414931904\n",
      "val l2_norm:  0.7074903051058451\n",
      "********** epoch:  146 **********\n",
      "train l2_norm:  0.901390858671882\n",
      "val l2_norm:  0.7065785825252533\n",
      "********** epoch:  147 **********\n",
      "train l2_norm:  0.9059377827427604\n",
      "val l2_norm:  0.7061362266540527\n",
      "********** epoch:  148 **********\n",
      "train l2_norm:  0.9079196832396768\n",
      "val l2_norm:  0.7103747924168905\n",
      "********** epoch:  149 **********\n",
      "train l2_norm:  0.9073800173672762\n",
      "val l2_norm:  0.7092012961705526\n",
      "********** epoch:  150 **********\n",
      "train l2_norm:  0.904997776855122\n",
      "val l2_norm:  0.7056622902552286\n",
      "********** epoch:  151 **********\n",
      "train l2_norm:  0.9029693603515625\n",
      "val l2_norm:  0.7060598929723104\n",
      "********** epoch:  152 **********\n",
      "train l2_norm:  0.9002381644465707\n",
      "val l2_norm:  0.7089022696018219\n",
      "********** epoch:  153 **********\n",
      "train l2_norm:  0.9007027040828358\n",
      "val l2_norm:  0.7056050002574921\n",
      "********** epoch:  154 **********\n",
      "train l2_norm:  0.9022488512776115\n",
      "val l2_norm:  0.7064919670422872\n",
      "********** epoch:  155 **********\n",
      "train l2_norm:  0.9057095132090829\n",
      "val l2_norm:  0.7120952208836874\n",
      "********** epoch:  156 **********\n",
      "train l2_norm:  0.9112256142226133\n",
      "val l2_norm:  0.7148989339669546\n",
      "********** epoch:  157 **********\n",
      "train l2_norm:  0.9156654612584547\n",
      "val l2_norm:  0.7119415899117788\n",
      "********** epoch:  158 **********\n",
      "train l2_norm:  0.918404611674222\n",
      "val l2_norm:  0.7114144265651703\n",
      "********** epoch:  159 **********\n",
      "train l2_norm:  0.9194183539260518\n",
      "val l2_norm:  0.711837500333786\n",
      "********** epoch:  160 **********\n",
      "train l2_norm:  0.9183008454062722\n",
      "val l2_norm:  0.7063270509243011\n",
      "********** epoch:  161 **********\n",
      "train l2_norm:  0.9160455600781874\n",
      "val l2_norm:  0.7060394187768301\n",
      "********** epoch:  162 **********\n",
      "train l2_norm:  0.9156785986640237\n",
      "val l2_norm:  0.7078214089075724\n",
      "********** epoch:  163 **********\n",
      "train l2_norm:  0.9181359003890645\n",
      "val l2_norm:  0.7017483214537302\n",
      "********** epoch:  164 **********\n",
      "train l2_norm:  0.9175313684073362\n",
      "val l2_norm:  0.7067875464757284\n",
      "********** epoch:  165 **********\n",
      "train l2_norm:  0.9155228165062991\n",
      "val l2_norm:  0.7046729624271393\n",
      "********** epoch:  166 **********\n",
      "train l2_norm:  0.9146321930668571\n",
      "val l2_norm:  0.7002865374088287\n",
      "********** epoch:  167 **********\n",
      "train l2_norm:  0.9153276140039618\n",
      "val l2_norm:  0.7046882510185242\n",
      "********** epoch:  168 **********\n",
      "train l2_norm:  0.9158717014572837\n",
      "val l2_norm:  0.7029840548833212\n",
      "********** epoch:  169 **********\n",
      "train l2_norm:  0.9132185876369476\n",
      "val l2_norm:  0.7090246578057607\n",
      "********** epoch:  170 **********\n",
      "train l2_norm:  0.9152509651400826\n",
      "val l2_norm:  0.7070072491963705\n",
      "********** epoch:  171 **********\n",
      "train l2_norm:  0.9103032865307548\n",
      "val l2_norm:  0.7067817747592926\n",
      "********** epoch:  172 **********\n",
      "train l2_norm:  0.9121848317709836\n",
      "val l2_norm:  0.7017156680425009\n",
      "********** epoch:  173 **********\n",
      "train l2_norm:  0.916812086647207\n",
      "val l2_norm:  0.7047602037588755\n",
      "********** epoch:  174 **********\n",
      "train l2_norm:  0.9197005927562714\n",
      "val l2_norm:  0.7127818663914999\n",
      "********** epoch:  175 **********\n",
      "train l2_norm:  0.9216371449557218\n",
      "val l2_norm:  0.7083396911621094\n",
      "********** epoch:  176 **********\n",
      "train l2_norm:  0.922536164522171\n",
      "val l2_norm:  0.7101327776908875\n",
      "********** epoch:  177 **********\n",
      "train l2_norm:  0.9235090938481417\n",
      "val l2_norm:  0.7065362930297852\n",
      "********** epoch:  178 **********\n",
      "train l2_norm:  0.9226886602965269\n",
      "val l2_norm:  0.7062894701957703\n",
      "********** epoch:  179 **********\n",
      "train l2_norm:  0.9220946539532054\n",
      "val l2_norm:  0.7084409793217977\n",
      "********** epoch:  180 **********\n",
      "train l2_norm:  0.9190749200907621\n",
      "val l2_norm:  0.7009975214799246\n",
      "********** epoch:  181 **********\n",
      "train l2_norm:  0.9122060970826582\n",
      "val l2_norm:  0.7065732777118683\n",
      "********** epoch:  182 **********\n",
      "train l2_norm:  0.914606435732408\n",
      "val l2_norm:  0.7066149115562439\n",
      "********** epoch:  183 **********\n",
      "train l2_norm:  0.9135815149003809\n",
      "val l2_norm:  0.7084317207336426\n",
      "********** epoch:  184 **********\n",
      "train l2_norm:  0.9139908281239596\n",
      "val l2_norm:  0.703803022702535\n",
      "********** epoch:  185 **********\n",
      "train l2_norm:  0.9146718762137673\n",
      "val l2_norm:  0.7057789166768392\n",
      "********** epoch:  186 **********\n",
      "train l2_norm:  0.9212489371949976\n",
      "val l2_norm:  0.7102246284484863\n",
      "********** epoch:  187 **********\n",
      "train l2_norm:  0.9256507456302643\n",
      "val l2_norm:  0.7134939134120941\n",
      "********** epoch:  188 **********\n",
      "train l2_norm:  0.9298629787835208\n",
      "val l2_norm:  0.7155971229076385\n",
      "********** epoch:  189 **********\n",
      "train l2_norm:  0.9357644725929607\n",
      "val l2_norm:  0.7147306700547537\n",
      "********** epoch:  190 **********\n",
      "train l2_norm:  0.942271576686339\n",
      "val l2_norm:  0.7138720353444418\n",
      "********** epoch:  191 **********\n",
      "train l2_norm:  0.9487790275703777\n",
      "val l2_norm:  0.7133453786373138\n",
      "********** epoch:  192 **********\n",
      "train l2_norm:  0.9532914893193678\n",
      "val l2_norm:  0.7124675214290619\n",
      "********** epoch:  193 **********\n",
      "train l2_norm:  0.9548578343608163\n",
      "val l2_norm:  0.7129324674606323\n",
      "********** epoch:  194 **********\n",
      "train l2_norm:  0.9578659669919447\n",
      "val l2_norm:  0.7129770815372467\n",
      "********** epoch:  195 **********\n",
      "train l2_norm:  0.9603762843392112\n",
      "val l2_norm:  0.7092707951863607\n",
      "********** epoch:  196 **********\n",
      "train l2_norm:  0.9609800929372961\n",
      "val l2_norm:  0.7129557828108469\n",
      "********** epoch:  197 **********\n",
      "train l2_norm:  0.9588050246238708\n",
      "val l2_norm:  0.7132544914881388\n",
      "********** epoch:  198 **********\n",
      "train l2_norm:  0.9596961167725649\n",
      "val l2_norm:  0.712131271759669\n",
      "********** epoch:  199 **********\n",
      "train l2_norm:  0.959124435078014\n",
      "val l2_norm:  0.7076119581858317\n",
      "Maximum Valid metric:  0.7155971229076385\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter()\n",
    "'''\n",
    "Train\n",
    "'''\n",
    "n_iter = 0\n",
    "val_loss = []\n",
    "for epoch in range(NUM_EPOCHS):# NUM_EPOCHS = 125\n",
    "    print('*'*10, 'epoch: ', epoch, '*'*10)\n",
    "    for phase in ['train', 'valid']:\n",
    "        n_iter +=1\n",
    "        if phase == 'train':\n",
    "            loss_list = []\n",
    "            model.train()\n",
    "            for i, data in enumerate(data_loaders[phase]):\n",
    "                input, label, depth = data\n",
    "                input = input.to(device)\n",
    "                label = label.to(device)\n",
    "                depth = depth.to(device)\n",
    "                output = model(input)\n",
    "                loss = l2_combo_loss(output, label, depth)\n",
    "                metric = IoU_metric(output, label)\n",
    "                \n",
    "                writer.add_scalar('Loss/train', loss.item(), n_iter)\n",
    "                writer.add_scalar('metric/train', metric.item(), n_iter)\n",
    "                \n",
    "                loss_list.append(metric.item())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            mean_loss = sum(loss_list) / len(loss_list)\n",
    "            print(\"train l2_norm: \", mean_loss)\n",
    "        elif phase == 'valid':\n",
    "            loss_list = []\n",
    "            model.eval()\n",
    "            for i, data in enumerate(data_loaders[phase]):\n",
    "                input, label, depth = data\n",
    "                input = input.to(device)\n",
    "                label = label.to(device)\n",
    "                depth = depth.to(device)\n",
    "                output = model(input)\n",
    "                loss = l2_combo_loss(output, label, depth)\n",
    "                metric = IoU_metric(output, label)\n",
    "                \n",
    "                writer.add_scalar('Loss/valid', loss.item(), n_iter)\n",
    "                writer.add_scalar('metric/valid', metric.item(), n_iter)\n",
    "                \n",
    "                loss_list.append(metric.item())\n",
    "            mean_loss = sum(loss_list) / len(loss_list)\n",
    "            print(\"val l2_norm: \", mean_loss)\n",
    "            val_loss.append(mean_loss)\n",
    "print('Maximum Valid metric: ', max(val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-11 11:11:24.171712: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2020-02-11 11:11:24.171993: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2020-02-11 11:11:24.172062: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.1.0 at http://localhost:6007/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.7703e-01, 2.8918e-08, 1.7072e-09, 7.7111e-02, 1.8839e-08, 4.5900e-09,\n",
       "        3.5316e-03, 6.1111e-09, 7.0097e-04, 1.3813e-06, 2.1142e-07, 4.3508e-06,\n",
       "        2.4897e-01, 9.9997e-01, 9.9966e-01, 9.1171e-01, 8.4263e-01, 8.3174e-01,\n",
       "        9.9983e-01, 9.8747e-01, 6.8330e-03, 9.1154e-06, 7.6345e-10, 2.3452e-08,\n",
       "        2.6252e-08, 3.0201e-09, 2.5496e-10, 1.9327e-11, 5.2191e-08, 5.2784e-04,\n",
       "        8.2549e-01, 4.1909e-03, 3.3774e-06, 9.5273e-03, 9.9988e-01, 8.3972e-01,\n",
       "        8.3446e-01, 7.9514e-01, 5.8471e-02, 4.7395e-04, 4.6276e-06, 3.1943e-06,\n",
       "        1.7269e-07, 2.1451e-07, 1.2743e-04, 7.7615e-01, 9.3710e-01, 4.6176e-04,\n",
       "        3.8613e-05, 8.0075e-08, 6.4346e-08, 9.1554e-09, 1.7823e-08, 8.2000e-01,\n",
       "        9.9999e-01, 2.6430e-07, 3.8870e-09, 6.0201e-05, 1.0114e-03, 1.4110e-05,\n",
       "        8.7925e-08, 1.6910e-08, 3.5990e-10, 1.0909e-09, 3.5481e-07, 4.4454e-05,\n",
       "        5.6372e-03, 2.5860e-01, 1.2867e-02, 8.6562e-05, 7.2968e-06, 1.0551e-06,\n",
       "        1.8678e-06, 6.1050e-05, 4.4887e-02, 5.2581e-02, 1.1609e-01, 9.6488e-01,\n",
       "        9.0613e-01, 3.6754e-03, 5.4540e-04, 1.8251e-03, 3.8064e-03, 1.2513e-02,\n",
       "        9.3008e-02, 9.9630e-01, 9.9458e-01, 7.3210e-01, 9.4915e-01, 9.3115e-01,\n",
       "        9.9299e-01, 9.9845e-01, 9.9947e-01, 1.0000e+00, 9.9994e-01, 9.2558e-01,\n",
       "        1.2962e-01, 1.2394e-02, 5.3179e-02, 4.8824e-01, 6.5996e-04, 6.8563e-03,\n",
       "        9.6400e-01, 9.9999e-01, 8.8458e-01, 1.8812e-01, 2.3313e-04, 2.4572e-07,\n",
       "        1.2334e-04, 3.8438e-01, 9.8861e-01, 9.8877e-01, 6.3733e-04, 2.5271e-07,\n",
       "        4.8078e-08, 2.3671e-05, 1.3011e-01, 4.5449e-02, 2.5003e-03, 2.8396e-02,\n",
       "        6.2398e-07, 5.2794e-12, 2.2986e-19, 2.4229e-17, 2.3047e-03, 4.3510e-02,\n",
       "        7.7725e-03, 9.8942e-04], device='cuda:1', grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(output[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=128x128 at 0x7FFF76260710>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_pil((output[0][0].cpu()>0).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/headless/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=128x128 at 0x7FFEE48659D0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_pil(F.sigmoid(output[0][1].cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showw(object, i):\n",
    "    imgs = object[i].cpu()\n",
    "    img = F.sigmoid(imgs[2][1])\n",
    "    return to_pil(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_inp = []\n",
    "list_out = []\n",
    "for i, data in enumerate(test_loader):\n",
    "    input = data\n",
    "    input = input.to(device)\n",
    "    test_output = model(input)\n",
    "    list_inp.append(input)\n",
    "    list_out.append(test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageSequence\n",
    "index = 3 \n",
    "for i in range(len(list_out)):\n",
    "    test_out = showw(list_out, i)\n",
    "    test_out.save(\"../r_unet/data/test_output/frame%d.png\" % index)\n",
    "    index += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
