{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['images_val', 'labels_val', 'test', 'labels', 'images']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import scipy.ndimage.morphology as morph\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "to_pil = transforms.ToPILImage()\n",
    "os.listdir('../r_unet/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = \"cuda:1\"\n",
    "# arguments\n",
    "TIMESTEPS = 3\n",
    "BATCH_SIZE = 1\n",
    "NUM_EPOCHS = 200\n",
    "INPUT_SIZE = 128\n",
    "INPUT_CHANNELS = 1\n",
    "NUM_CLASSES = 2\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "RECURRENT = True\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                              transforms.Resize((INPUT_SIZE, INPUT_SIZE), interpolation = 0),\n",
    "                              transforms.ToTensor()\n",
    "                              ])\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "# decive\n",
    "device = torch.device(DEVICE if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# way to the data folders\n",
    "FOLDER_DATA = \"../r_unet/data/images\"\n",
    "FOLDER_MASK = \"../r_unet/data/labels\"\n",
    "FOLDER_TEST = \"../r_unet/data/test\"\n",
    "FOLDER_DATA_VAL = \"../r_unet/data/images_val\"\n",
    "FOLDER_MASK_VAL = \"../r_unet/data/labels_val\"\n",
    "\n",
    "FILE_NAMES = sorted(os.listdir('../r_unet/data/images'))\n",
    "FILE_NAMES_VAL = sorted(os.listdir('../r_unet/data/images_val'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(object):\n",
    "    label1 = (object==0).float()\n",
    "    label2 = (label1==0).float()\n",
    "    labels = torch.stack([label1, label2], dim=1).squeeze()\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainMedData(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.time = TIMESTEPS\n",
    "        self.folder_data = FOLDER_DATA\n",
    "        self.folder_mask = FOLDER_MASK\n",
    "        self.file_names = FILE_NAMES\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        gif_list = []\n",
    "        for i in range(self.time):\n",
    "            gif_list.append(transform(Image.open(self.folder_data + '/' + self.file_names[idx+i])))\n",
    "        gif_data = torch.stack(gif_list)\n",
    "        gif_list.clear()\n",
    "        for i in range(self.time):\n",
    "            gif_list.append(get_labels(transform(Image.open(self.folder_mask + '/' + self.file_names[idx+i]))))\n",
    "        gif_mask = torch.stack(gif_list)\n",
    "        gif_list.clear()\n",
    "        for i in range(self.time):\n",
    "            img = Image.open(self.folder_mask + '/' + self.file_names[idx+i])\n",
    "            img = img.resize((INPUT_SIZE, INPUT_SIZE), resample=Image.NEAREST)\n",
    "            gif_list.append(to_tensor(morph.distance_transform_edt(np.asarray(img)/255)))\n",
    "        gif_depth = torch.stack(gif_list)\n",
    "        return gif_data, gif_mask, gif_depth\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_names) - self.time + 1\n",
    "\n",
    "\n",
    "class ValMedData(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.time = TIMESTEPS\n",
    "        self.folder_data = FOLDER_DATA_VAL\n",
    "        self.folder_mask = FOLDER_MASK_VAL\n",
    "        self.file_names = FILE_NAMES_VAL\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        gif_list = []\n",
    "        for i in range(self.time):\n",
    "            gif_list.append(transform(Image.open(self.folder_data + '/' + self.file_names[idx+i])))\n",
    "        gif_data = torch.stack(gif_list)\n",
    "        gif_list.clear()\n",
    "        for i in range(self.time):\n",
    "            gif_list.append(get_labels(transform(Image.open(self.folder_mask + '/' + self.file_names[idx+i]))))\n",
    "        gif_mask = torch.stack(gif_list).squeeze(dim=2)\n",
    "        gif_list.clear()\n",
    "        for i in range(self.time):\n",
    "            img = Image.open(self.folder_mask + '/' + self.file_names[idx+i])\n",
    "            img = img.resize((INPUT_SIZE, INPUT_SIZE), resample=Image.NEAREST)\n",
    "            gif_list.append(to_tensor(morph.distance_transform_edt(np.asarray(img)/255)))\n",
    "        gif_depth = torch.stack(gif_list)\n",
    "        return gif_data, gif_mask, gif_depth\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names) - self.time + 1\n",
    "\n",
    "\n",
    "class TestMedData(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__\n",
    "        self.time = TIMESTEPS\n",
    "        self.folder_test = FOLDER_TEST\n",
    "        self.file_names = FILE_NAMES + FILE_NAMES_VAL\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        gif_list = []\n",
    "        for i in range(self.time):\n",
    "            gif_list.append(transform(Image.open(self.folder_test + '/' + self.file_names[idx+i])))\n",
    "        gif_test = torch.stack(gif_list)\n",
    "        gif_list.clear()\n",
    "        return gif_test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names) - self.time + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrainMedData()\n",
    "valid_dataset = ValMedData()\n",
    "test_dataset = TestMedData()\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          num_workers=1,\n",
    "                          shuffle=False)\n",
    "\n",
    "valid_loader = DataLoader(dataset=valid_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          num_workers=1,\n",
    "                          shuffle=False)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=1,\n",
    "                         num_workers=1,\n",
    "                         shuffle=False)\n",
    "\n",
    "data_loaders = {\n",
    "    'train' : train_loader,\n",
    "    'valid' : valid_loader,\n",
    "    'test' : test_loader\n",
    "}\n",
    "\n",
    "dataset_sizes = {\n",
    "    'train': len(train_dataset),\n",
    "    'valid': len(valid_dataset),\n",
    "    'test': len(test_dataset)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 128, 128])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y, z =train_dataset[0]\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvRnnCell(nn.Module):    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvRnnCell, self).__init__()\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(in_channels+out_channels, out_channels, kernel_size=3, padding=1))\n",
    "             \n",
    "    def forward(self, x, hidden):\n",
    "        out = torch.cat([x, hidden],dim=1)\n",
    "        out = self.conv1(out)\n",
    "        hidden = out\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvGruCell(nn.Module):    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvGruCell, self).__init__()\n",
    "        self.conv_relu_for_input = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        \n",
    "        self.conv_relu_for_hidden = nn.Sequential(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))\n",
    "        \n",
    "        self.conv_relu_2x_update = nn.Sequential(nn.Conv2d(in_channels+out_channels, out_channels, kernel_size=3, padding=1))\n",
    "        self.conv_relu_2x_reset = nn.Sequential(nn.Conv2d(in_channels+out_channels, out_channels, kernel_size=3, padding=1))\n",
    "        \n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "     \n",
    "    def forward(self, x, hidden):\n",
    "        input = torch.cat([x, hidden],dim=1)\n",
    "\n",
    "        update_gate = self.conv_relu_2x_update(input)\n",
    "        update_gate = self.relu((update_gate)) ### output after update gate\n",
    "        reset_gate = self.conv_relu_2x_reset(input)\n",
    "        reset_gate = self.relu((reset_gate)) ### output after reset gate\n",
    "        \n",
    "        \n",
    "        memory_gate_for_input = self.conv_relu_for_input(x)\n",
    "        memory_gate_for_hidden = self.conv_relu_for_hidden(hidden)\n",
    "\n",
    "        memory_content = memory_gate_for_input + (reset_gate * memory_gate_for_hidden) ### output for reset gate(affects how the reset gate do work)\n",
    "        memory_content = self.relu(memory_content)\n",
    "\n",
    "        hidden = (update_gate * hidden) + ((1 - update_gate) * memory_content) # torch.ones(input_size, hidden_size)\n",
    "\n",
    "        return hidden, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvRnn(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, ConvRnn_input_size): # arg for ConvRnn layer\n",
    "        super(ConvRnn, self).__init__()\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.timesteps = TIMESTEPS\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.input_size = ConvRnn_input_size\n",
    "        self.hidden_size = (self.batch_size, self.out_channels, self.input_size, self.input_size)\n",
    "        \n",
    "        self.ConvRnn_layer = ConvRnnCell(in_channels, out_channels)\n",
    "        self.init_hidden = torch.zeros(self.hidden_size).to(device)\n",
    "        self.rec = RECURRENT\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_cells = None\n",
    "        x_list = []\n",
    "\n",
    "        x = x.reshape(self.batch_size, self.timesteps, self.in_channels, self.input_size, self.input_size)\n",
    "        x = x.permute(1, 0, 2, 3, 4)\n",
    "        for i in range(self.timesteps):\n",
    "            if x_cells is None:\n",
    "                x_cells, hidden = self.ConvRnn_layer(x[i], self.init_hidden)\n",
    "                x_list.append(x_cells)\n",
    "            else:\n",
    "                x_i, hidden = self.ConvRnn_layer(x[i], hidden)\n",
    "                x_list.append(x_i)\n",
    "        x_cells = torch.stack(x_list)\n",
    "        x_cells = x_cells.permute(1, 0, 2, 3, 4)\n",
    "\n",
    "        x_cells = x_cells.reshape(-1, self.out_channels, self.input_size, self.input_size)\n",
    "        return x_cells  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvRnnRelu(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels ,ConvRnn_input_size):\n",
    "        super(ConvRnnRelu, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.input_size = ConvRnn_input_size\n",
    "        self.convrnnrelu = nn.Sequential(ConvRnn(self.in_channels, self.out_channels, self.input_size),\n",
    "                                         nn.ReLU()\n",
    "                                         )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.convrnnrelu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvRelu(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvRelu, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.convrelu = nn.Sequential(nn.Conv2d(self.in_channels, self.out_channels, kernel_size=3, padding=1),\n",
    "                                      nn.ReLU()\n",
    "                                      )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.convrelu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaxPool, self).__init__()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.maxpool(x)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpAndCat(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(UpAndCat, self).__init__()\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "\n",
    "    def forward(self, x_up, x_cat):\n",
    "        out = self.up(x_up)\n",
    "        out = torch.cat([out, x_cat], dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetDesigner(nn.Module):    \n",
    "    def __init__(self, d1, d2, d3, b, u1, u2, u3,\n",
    "                 input_size=INPUT_SIZE, input_channels=INPUT_CHANNELS, num_classes=NUM_CLASSES):\n",
    "        super(UNetDesigner, self).__init__()\n",
    "        self.num_classes = NUM_CLASSES\n",
    "        self.d1, self.d2, self.d3, self.b, self.u1, self.u2, self.u3 = d1, d2, d3, b, u1, u2, u3\n",
    "        self.input_size = input_size\n",
    "        self.input_chennels = input_channels\n",
    "        self.ch_list = [self.input_chennels, 32, 64, 128, 256]\n",
    "        self.input_x2 = int(self.input_size / 2)\n",
    "        self.input_x4 = int(self.input_size / 4)\n",
    "        self.input_x8 = int(self.input_size / 8)\n",
    "         # Down_1 layer\n",
    "        if self.d1 == True:                                                       #input_size = 128      # Channels\n",
    "            self.down1 = nn.Sequential(ConvRnnRelu(self.ch_list[0], self.ch_list[1], self.input_size),   # 1  -->32\n",
    "                                       ConvRelu(self.ch_list[1], self.ch_list[1])                        # 32 -->32\n",
    "                                       )\n",
    "        else:    \n",
    "            self.down1 = nn.Sequential(ConvRelu(self.ch_list[0], self.ch_list[1]),                       # 1  -->32\n",
    "                                       ConvRelu(self.ch_list[1], self.ch_list[1])                        # 32 -->32\n",
    "                                       )\n",
    "        self.down1_pool = MaxPool()\n",
    "         # Down_2 layer                                                           #input_size = 64\n",
    "        if self.d2 == True:\n",
    "            self.down2 = nn.Sequential(ConvRnnRelu(self.ch_list[1], self.ch_list[2], self.input_x2),     # 32 -->64\n",
    "                                       ConvRelu(self.ch_list[2], self.ch_list[2])                        # 64 -->64\n",
    "                                       )\n",
    "        else:\n",
    "            self.down2 = nn.Sequential(ConvRelu(self.ch_list[1], self.ch_list[2]),                       # 32 -->64\n",
    "                                       ConvRelu(self.ch_list[2], self.ch_list[2])                        # 64 -->64\n",
    "                                       )\n",
    "        self.down2_pool = MaxPool()\n",
    "         # Down_3 layer                                                           #input_size = 32\n",
    "        if self.d3 == True:\n",
    "            self.down3 = nn.Sequential(ConvRnnRelu(self.ch_list[2], self.ch_list[3], self.input_x4),     # 64 -->128\n",
    "                                       ConvRelu(self.ch_list[3], self.ch_list[3])                        # 128-->128\n",
    "                                       )\n",
    "        else:\n",
    "            self.down3 = nn.Sequential(ConvRelu(self.ch_list[2], self.ch_list[3]),                       # 64 -->128\n",
    "                                       ConvRelu(self.ch_list[3], self.ch_list[3])                        # 128-->128\n",
    "                                       )\n",
    "        self.down3_pool = MaxPool()\n",
    "         # Bottom layer                                                           #input_size = 16\n",
    "        if self.b == True:\n",
    "            self.bottom = nn.Sequential(ConvRnnRelu(self.ch_list[3], self.ch_list[4], self.input_x8),    # 128-->256\n",
    "                                        ConvRelu(self.ch_list[4], self.ch_list[4]),                      # 256-->256\n",
    "                                        )\n",
    "        else:\n",
    "            self.bottom = nn.Sequential(ConvRelu(self.ch_list[3], self.ch_list[4]),                      # 128-->256\n",
    "                                        ConvRelu(self.ch_list[4], self.ch_list[4]),                      # 256-->256\n",
    "                                        )\n",
    "         # Up_3 layer\n",
    "        self.up_cat_3 = UpAndCat()\n",
    "        self.up_conv_3 = nn.Sequential(ConvRelu(self.ch_list[4]+self.ch_list[3], self.ch_list[3]),       # 394-->128\n",
    "                                       ConvRelu(self.ch_list[3], self.ch_list[3])                        # 128-->128\n",
    "                                       )\n",
    "         # Up_2 layer\n",
    "        self.up_cat_2 = UpAndCat()\n",
    "        self.up_conv_2 = nn.Sequential(ConvRelu(self.ch_list[3]+self.ch_list[2], self.ch_list[2]),       # 192-->64\n",
    "                                       ConvRelu(self.ch_list[2], self.ch_list[2])                        # 64 -->64\n",
    "                                       )\n",
    "         # Up_1 layer\n",
    "        self.up_cat_1 = UpAndCat()\n",
    "        self.up_conv_1 = nn.Sequential(ConvRelu(self.ch_list[2]+self.ch_list[1], self.ch_list[1]),       # 96 -->32\n",
    "                                       ConvRelu(self.ch_list[1], self.ch_list[1])                        # 32 -->32\n",
    "                                       )\n",
    "         # Final layer\n",
    "        self.final = nn.Sequential(nn.Conv2d(self.ch_list[1], self.num_classes, kernel_size=1),\n",
    "                                   nn.Sigmoid(),\n",
    "                                   )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, self.input_chennels, self.input_size, self.input_size)\n",
    "        # print(x.shape)\n",
    "        down1_feat = self.down1(x)\n",
    "        pool1 = self.down1_pool(down1_feat)\n",
    "        # print(pool1.shape)\n",
    "        down2_feat = self.down2(pool1)\n",
    "        pool2 = self.down2_pool(down2_feat)\n",
    "        # print(pool2.shape)\n",
    "        down3_feat = self.down3(pool2)\n",
    "        pool3 = self.down3_pool(down3_feat)\n",
    "        # print(pool3.shape)\n",
    "        bottom_feat = self.bottom(pool3)\n",
    "        # print(bottom_feat.shape)\n",
    "        up_feat3 = self.up_cat_3(bottom_feat, down3_feat)\n",
    "        up_feat3 = self.up_conv_3(up_feat3)\n",
    "        \n",
    "        up_feat2 = self.up_cat_2(up_feat3, down2_feat)\n",
    "        up_feat2 = self.up_conv_2(up_feat2)\n",
    "        \n",
    "        up_feat1 = self.up_cat_1(up_feat2, down1_feat)\n",
    "        up_feat1 = self.up_conv_1(up_feat1)\n",
    "        \n",
    "        out = self.final(up_feat1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetGruAll(nn.Module):    \n",
    "    def __init__(self, d1, d2, d3, b, u1, u2, u3,\n",
    "                 input_size=INPUT_SIZE, input_channels=INPUT_CHANNELS, num_classes=NUM_CLASSES):\n",
    "        super(UNetGruAll, self).__init__()\n",
    "        self.num_classes = NUM_CLASSES\n",
    "        self.d1, self.d2, self.d3, self.b, self.u1, self.u2, self.u3 = d1, d2, d3, b, u1, u2, u3\n",
    "        self.input_size = input_size\n",
    "        self.input_chennels = input_channels\n",
    "        self.ch_list = [self.input_chennels, 32, 64, 128, 256]\n",
    "        self.input_x2 = int(self.input_size / 2)\n",
    "        self.input_x4 = int(self.input_size / 4)\n",
    "        self.input_x8 = int(self.input_size / 8)\n",
    "         # Down_1 layer\n",
    "        if self.d1 == True:                                                       #input_size = 128      # Channels\n",
    "            self.down1 = nn.Sequential(ConvRnnRelu(self.ch_list[0], self.ch_list[1], self.input_size),   # 1  -->32\n",
    "                                       ConvRnnRelu(self.ch_list[1], self.ch_list[1], self.input_size)                        # 32 -->32\n",
    "                                       )\n",
    "        else:    \n",
    "            self.down1 = nn.Sequential(ConvRelu(self.ch_list[0], self.ch_list[1]),                       # 1  -->32\n",
    "                                       ConvRelu(self.ch_list[1], self.ch_list[1])                        # 32 -->32\n",
    "                                       )\n",
    "        self.down1_pool = MaxPool()\n",
    "         # Down_2 layer                                                           #input_size = 64\n",
    "        if self.d2 == True:\n",
    "            self.down2 = nn.Sequential(ConvRnnRelu(self.ch_list[1], self.ch_list[2], self.input_x2),   # 1  -->32\n",
    "                                       ConvRnnRelu(self.ch_list[2], self.ch_list[2], self.input_x2)                        # 64 -->64\n",
    "                                       )\n",
    "        else:\n",
    "            self.down2 = nn.Sequential(ConvRelu(self.ch_list[1], self.ch_list[2]),                       # 32 -->64\n",
    "                                       ConvRelu(self.ch_list[2], self.ch_list[2])                        # 64 -->64\n",
    "                                       )\n",
    "        self.down2_pool = MaxPool()\n",
    "         # Down_3 layer                                                           #input_size = 32\n",
    "        if self.d3 == True:\n",
    "            self.down3 = nn.Sequential(ConvRnnRelu(self.ch_list[2], self.ch_list[3], self.input_x4),   # 1  -->32\n",
    "                                       ConvRnnRelu(self.ch_list[3], self.ch_list[3], self.input_x4)                        # 128-->128\n",
    "                                       )\n",
    "        else:\n",
    "            self.down3 = nn.Sequential(ConvRelu(self.ch_list[2], self.ch_list[3]),                       # 64 -->128\n",
    "                                       ConvRelu(self.ch_list[3], self.ch_list[3])                        # 128-->128\n",
    "                                       )\n",
    "        self.down3_pool = MaxPool()\n",
    "         # Bottom layer                                                           #input_size = 16\n",
    "        if self.b == True:\n",
    "            self.bottom = nn.Sequential(ConvRnnRelu(self.ch_list[3], self.ch_list[4], self.input_x8),   # 1  -->32\n",
    "                                       ConvRnnRelu(self.ch_list[4], self.ch_list[4], self.input_x8)                     # 256-->256\n",
    "                                        )\n",
    "        else:\n",
    "            self.bottom = nn.Sequential(ConvRelu(self.ch_list[3], self.ch_list[4]),                      # 128-->256\n",
    "                                        ConvRelu(self.ch_list[4], self.ch_list[4]),                      # 256-->256\n",
    "                                        )\n",
    "         # Up_3 layer\n",
    "        self.up_cat_3 = UpAndCat()\n",
    "        self.up_conv_3 = nn.Sequential(ConvRelu(self.ch_list[4]+self.ch_list[3], self.ch_list[3]),       # 394-->128\n",
    "                                       ConvRelu(self.ch_list[3], self.ch_list[3])                        # 128-->128\n",
    "                                       )\n",
    "         # Up_2 layer\n",
    "        self.up_cat_2 = UpAndCat()\n",
    "        self.up_conv_2 = nn.Sequential(ConvRelu(self.ch_list[3]+self.ch_list[2], self.ch_list[2]),       # 192-->64\n",
    "                                       ConvRelu(self.ch_list[2], self.ch_list[2])                        # 64 -->64\n",
    "                                       )\n",
    "         # Up_1 layer\n",
    "        self.up_cat_1 = UpAndCat()\n",
    "        self.up_conv_1 = nn.Sequential(ConvRelu(self.ch_list[2]+self.ch_list[1], self.ch_list[1]),       # 96 -->32\n",
    "                                       ConvRelu(self.ch_list[1], self.ch_list[1])                        # 32 -->32\n",
    "                                       )\n",
    "         # Final layer\n",
    "        self.final = nn.Sequential(nn.Conv2d(self.ch_list[1], self.num_classes, kernel_size=1),\n",
    "                                   nn.Sigmoid(),\n",
    "                                   )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, self.input_chennels, self.input_size, self.input_size)\n",
    "        # print(x.shape)\n",
    "        down1_feat = self.down1(x)\n",
    "        pool1 = self.down1_pool(down1_feat)\n",
    "        # print(pool1.shape)\n",
    "        down2_feat = self.down2(pool1)\n",
    "        pool2 = self.down2_pool(down2_feat)\n",
    "        # print(pool2.shape)\n",
    "        down3_feat = self.down3(pool2)\n",
    "        pool3 = self.down3_pool(down3_feat)\n",
    "        # print(pool3.shape)\n",
    "        bottom_feat = self.bottom(pool3)\n",
    "        # print(bottom_feat.shape)\n",
    "        up_feat3 = self.up_cat_3(bottom_feat, down3_feat)\n",
    "        up_feat3 = self.up_conv_3(up_feat3)\n",
    "        \n",
    "        up_feat2 = self.up_cat_2(up_feat3, down2_feat)\n",
    "        up_feat2 = self.up_conv_2(up_feat2)\n",
    "        \n",
    "        up_feat1 = self.up_cat_1(up_feat2, down1_feat)\n",
    "        up_feat1 = self.up_conv_1(up_feat1)\n",
    "        \n",
    "        out = self.final(up_feat1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2, 3, 1, 128, 128).to(device)\n",
    "model = UNetDesigner(d1=True, d2=True, d3=True, b=True, u1=1, u2=1, u3=1)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "def l2_norm(x, y):\n",
    "    y = y.reshape(x.shape)\n",
    "    return (((x - y)**2).sum(dim=2)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** epoch:  0 **********\n",
      "train l2_norm:  18913.45751953125\n",
      "val l2_norm:  15522.144205729166\n",
      "********** epoch:  1 **********\n",
      "train l2_norm:  16681.507057883522\n",
      "val l2_norm:  15130.650065104166\n",
      "********** epoch:  2 **********\n",
      "train l2_norm:  16008.262517755682\n",
      "val l2_norm:  14514.916666666666\n",
      "********** epoch:  3 **********\n",
      "train l2_norm:  15329.663751775568\n",
      "val l2_norm:  13699.045735677084\n",
      "********** epoch:  4 **********\n",
      "train l2_norm:  13738.120472301136\n",
      "val l2_norm:  12313.798177083334\n",
      "********** epoch:  5 **********\n",
      "train l2_norm:  12256.862571022728\n",
      "val l2_norm:  11197.837076822916\n",
      "********** epoch:  6 **********\n",
      "train l2_norm:  11286.586381392046\n",
      "val l2_norm:  10756.239095052084\n",
      "********** epoch:  7 **********\n",
      "train l2_norm:  11226.73086825284\n",
      "val l2_norm:  11331.393880208334\n",
      "********** epoch:  8 **********\n",
      "train l2_norm:  11404.334605823864\n",
      "val l2_norm:  12513.206380208334\n",
      "********** epoch:  9 **********\n",
      "train l2_norm:  10769.048872514204\n",
      "val l2_norm:  11165.485026041666\n",
      "********** epoch:  10 **********\n",
      "train l2_norm:  11044.500044389204\n",
      "val l2_norm:  11577.601888020834\n",
      "********** epoch:  11 **********\n",
      "train l2_norm:  10647.419389204546\n",
      "val l2_norm:  10719.385091145834\n",
      "********** epoch:  12 **********\n",
      "train l2_norm:  10639.258922230114\n",
      "val l2_norm:  10315.387044270834\n",
      "********** epoch:  13 **********\n",
      "train l2_norm:  10804.20352450284\n",
      "val l2_norm:  11002.353190104166\n",
      "********** epoch:  14 **********\n",
      "train l2_norm:  10365.314763849432\n",
      "val l2_norm:  10426.1181640625\n",
      "********** epoch:  15 **********\n",
      "train l2_norm:  10093.132634943182\n",
      "val l2_norm:  10241.1904296875\n",
      "********** epoch:  16 **********\n",
      "train l2_norm:  10029.645862926136\n",
      "val l2_norm:  10839.024088541666\n",
      "********** epoch:  17 **********\n",
      "train l2_norm:  9924.979714133522\n",
      "val l2_norm:  10723.239583333334\n",
      "********** epoch:  18 **********\n",
      "train l2_norm:  9860.936700994318\n",
      "val l2_norm:  10492.324544270834\n",
      "********** epoch:  19 **********\n",
      "train l2_norm:  9803.367453835228\n",
      "val l2_norm:  10602.694661458334\n",
      "********** epoch:  20 **********\n",
      "train l2_norm:  9976.559925426136\n",
      "val l2_norm:  10911.763020833334\n",
      "********** epoch:  21 **********\n",
      "train l2_norm:  9681.13037109375\n",
      "val l2_norm:  10426.272786458334\n",
      "********** epoch:  22 **********\n",
      "train l2_norm:  9512.127796519886\n",
      "val l2_norm:  10233.446451822916\n",
      "********** epoch:  23 **********\n",
      "train l2_norm:  9535.837713068182\n",
      "val l2_norm:  10212.63330078125\n",
      "********** epoch:  24 **********\n",
      "train l2_norm:  9422.127796519886\n",
      "val l2_norm:  10014.62109375\n",
      "********** epoch:  25 **********\n",
      "train l2_norm:  9526.829944957386\n",
      "val l2_norm:  10499.020345052084\n",
      "********** epoch:  26 **********\n",
      "train l2_norm:  9368.901655717329\n",
      "val l2_norm:  9850.420084635416\n",
      "********** epoch:  27 **********\n",
      "train l2_norm:  9116.405606356535\n",
      "val l2_norm:  9915.005696614584\n",
      "********** epoch:  28 **********\n",
      "train l2_norm:  9325.110617897728\n",
      "val l2_norm:  10119.877278645834\n",
      "********** epoch:  29 **********\n",
      "train l2_norm:  9118.447576349432\n",
      "val l2_norm:  9764.359049479166\n",
      "********** epoch:  30 **********\n",
      "train l2_norm:  8994.384632457386\n",
      "val l2_norm:  9817.690755208334\n",
      "********** epoch:  31 **********\n",
      "train l2_norm:  9244.23974609375\n",
      "val l2_norm:  9767.425130208334\n",
      "********** epoch:  32 **********\n",
      "train l2_norm:  9007.721968217329\n",
      "val l2_norm:  9491.10205078125\n",
      "********** epoch:  33 **********\n",
      "train l2_norm:  8801.482799183239\n",
      "val l2_norm:  9592.727864583334\n",
      "********** epoch:  34 **********\n",
      "train l2_norm:  8769.504594282671\n",
      "val l2_norm:  9532.112467447916\n",
      "********** epoch:  35 **********\n",
      "train l2_norm:  8687.524946732954\n",
      "val l2_norm:  9445.119466145834\n",
      "********** epoch:  36 **********\n",
      "train l2_norm:  8616.045942826704\n",
      "val l2_norm:  9485.178059895834\n",
      "********** epoch:  37 **********\n",
      "train l2_norm:  8504.20068359375\n",
      "val l2_norm:  8903.010579427084\n",
      "********** epoch:  38 **********\n",
      "train l2_norm:  8441.674582741478\n",
      "val l2_norm:  9447.0732421875\n",
      "********** epoch:  39 **********\n",
      "train l2_norm:  8427.491477272728\n",
      "val l2_norm:  9477.62158203125\n",
      "********** epoch:  40 **********\n",
      "train l2_norm:  8474.128107244318\n",
      "val l2_norm:  9354.846028645834\n",
      "********** epoch:  41 **********\n",
      "train l2_norm:  8417.209161931818\n",
      "val l2_norm:  9384.935384114584\n",
      "********** epoch:  42 **********\n",
      "train l2_norm:  8290.918190696022\n",
      "val l2_norm:  9307.8115234375\n",
      "********** epoch:  43 **********\n",
      "train l2_norm:  8199.267156427557\n",
      "val l2_norm:  9202.052734375\n",
      "********** epoch:  44 **********\n",
      "train l2_norm:  8154.724986683239\n",
      "val l2_norm:  9119.430501302084\n",
      "********** epoch:  45 **********\n",
      "train l2_norm:  8173.177867542614\n",
      "val l2_norm:  9167.79443359375\n",
      "********** epoch:  46 **********\n",
      "train l2_norm:  8036.556640625\n",
      "val l2_norm:  8828.439453125\n",
      "********** epoch:  47 **********\n",
      "train l2_norm:  8001.998801491477\n",
      "val l2_norm:  8840.013346354166\n",
      "********** epoch:  48 **********\n",
      "train l2_norm:  7942.130859375\n",
      "val l2_norm:  8821.860514322916\n",
      "********** epoch:  49 **********\n",
      "train l2_norm:  7890.434015447443\n",
      "val l2_norm:  8893.01171875\n",
      "********** epoch:  50 **********\n",
      "train l2_norm:  7838.4287109375\n",
      "val l2_norm:  8886.948893229166\n",
      "********** epoch:  51 **********\n",
      "train l2_norm:  7807.786887428977\n",
      "val l2_norm:  8969.267578125\n",
      "********** epoch:  52 **********\n",
      "train l2_norm:  7701.191850142045\n",
      "val l2_norm:  8902.204427083334\n",
      "********** epoch:  53 **********\n",
      "train l2_norm:  7612.424405184659\n",
      "val l2_norm:  8843.199055989584\n",
      "********** epoch:  54 **********\n",
      "train l2_norm:  7607.333385120739\n",
      "val l2_norm:  8848.085286458334\n",
      "********** epoch:  55 **********\n",
      "train l2_norm:  7572.467107599432\n",
      "val l2_norm:  8841.833170572916\n",
      "********** epoch:  56 **********\n",
      "train l2_norm:  7508.752974076705\n",
      "val l2_norm:  8747.838541666666\n",
      "********** epoch:  57 **********\n",
      "train l2_norm:  7489.617542613636\n",
      "val l2_norm:  8832.68701171875\n",
      "********** epoch:  58 **********\n",
      "train l2_norm:  7513.567049893466\n",
      "val l2_norm:  8822.855631510416\n",
      "********** epoch:  59 **********\n",
      "train l2_norm:  7486.452570134943\n",
      "val l2_norm:  8658.485026041666\n",
      "********** epoch:  60 **********\n",
      "train l2_norm:  7526.815451882102\n",
      "val l2_norm:  8484.864176432291\n",
      "********** epoch:  61 **********\n",
      "train l2_norm:  7465.787841796875\n",
      "val l2_norm:  8356.007568359375\n",
      "********** epoch:  62 **********\n",
      "train l2_norm:  7344.123579545455\n",
      "val l2_norm:  8395.460856119791\n",
      "********** epoch:  63 **********\n",
      "train l2_norm:  7278.417924360795\n",
      "val l2_norm:  8345.543375651041\n",
      "********** epoch:  64 **********\n",
      "train l2_norm:  7175.335427024148\n",
      "val l2_norm:  8417.006184895834\n",
      "********** epoch:  65 **********\n",
      "train l2_norm:  7142.939386541193\n",
      "val l2_norm:  8526.596435546875\n",
      "********** epoch:  66 **********\n",
      "train l2_norm:  7100.525368430398\n",
      "val l2_norm:  8528.677164713541\n",
      "********** epoch:  67 **********\n",
      "train l2_norm:  7140.518776633523\n",
      "val l2_norm:  8694.596354166666\n",
      "********** epoch:  68 **********\n",
      "train l2_norm:  7089.273193359375\n",
      "val l2_norm:  8651.37109375\n",
      "********** epoch:  69 **********\n",
      "train l2_norm:  7052.061834161932\n",
      "val l2_norm:  8631.377685546875\n",
      "********** epoch:  70 **********\n",
      "train l2_norm:  7099.877041903409\n",
      "val l2_norm:  8488.812337239584\n",
      "********** epoch:  71 **********\n",
      "train l2_norm:  7171.623690518466\n",
      "val l2_norm:  8269.204671223959\n",
      "********** epoch:  72 **********\n",
      "train l2_norm:  7113.555996981534\n",
      "val l2_norm:  8298.813313802084\n",
      "********** epoch:  73 **********\n",
      "train l2_norm:  7045.929643110795\n",
      "val l2_norm:  8145.591471354167\n",
      "********** epoch:  74 **********\n",
      "train l2_norm:  6917.300514914773\n",
      "val l2_norm:  8094.103759765625\n",
      "********** epoch:  75 **********\n",
      "train l2_norm:  6963.44462446733\n",
      "val l2_norm:  8042.849283854167\n",
      "********** epoch:  76 **********\n",
      "train l2_norm:  6880.533358487216\n",
      "val l2_norm:  7958.322184244792\n",
      "********** epoch:  77 **********\n",
      "train l2_norm:  6808.117520419034\n",
      "val l2_norm:  7979.413492838542\n",
      "********** epoch:  78 **********\n",
      "train l2_norm:  6753.057017933239\n",
      "val l2_norm:  7973.135091145833\n",
      "********** epoch:  79 **********\n",
      "train l2_norm:  6675.173273259943\n",
      "val l2_norm:  8026.339029947917\n",
      "********** epoch:  80 **********\n",
      "train l2_norm:  6591.960981889205\n",
      "val l2_norm:  8076.322184244792\n",
      "********** epoch:  81 **********\n",
      "train l2_norm:  6547.661798650568\n",
      "val l2_norm:  8315.832112630209\n",
      "********** epoch:  82 **********\n",
      "train l2_norm:  6549.78446821733\n",
      "val l2_norm:  8303.452718098959\n",
      "********** epoch:  83 **********\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train l2_norm:  6573.196222478693\n",
      "val l2_norm:  8416.929931640625\n",
      "********** epoch:  84 **********\n",
      "train l2_norm:  6500.090420809659\n",
      "val l2_norm:  8338.540201822916\n",
      "********** epoch:  85 **********\n",
      "train l2_norm:  6596.668235085227\n",
      "val l2_norm:  8296.771321614584\n",
      "********** epoch:  86 **********\n",
      "train l2_norm:  6571.725985440341\n",
      "val l2_norm:  7979.725260416667\n",
      "********** epoch:  87 **********\n",
      "train l2_norm:  6421.141268643466\n",
      "val l2_norm:  8036.558186848958\n",
      "********** epoch:  88 **********\n",
      "train l2_norm:  6512.70146040483\n",
      "val l2_norm:  7975.773274739583\n",
      "********** epoch:  89 **********\n",
      "train l2_norm:  6788.799183238636\n",
      "val l2_norm:  8072.3193359375\n",
      "********** epoch:  90 **********\n",
      "train l2_norm:  6638.535600142045\n",
      "val l2_norm:  7951.102620442708\n",
      "********** epoch:  91 **********\n",
      "train l2_norm:  6450.288219105114\n",
      "val l2_norm:  7854.757731119792\n",
      "********** epoch:  92 **********\n",
      "train l2_norm:  6366.109929865057\n",
      "val l2_norm:  7901.7734375\n",
      "********** epoch:  93 **********\n",
      "train l2_norm:  6274.752618963068\n",
      "val l2_norm:  7883.927164713542\n",
      "********** epoch:  94 **********\n",
      "train l2_norm:  6215.804354580966\n",
      "val l2_norm:  7854.40380859375\n",
      "********** epoch:  95 **********\n",
      "train l2_norm:  6128.700617009943\n",
      "val l2_norm:  7861.599039713542\n",
      "********** epoch:  96 **********\n",
      "train l2_norm:  6093.505681818182\n",
      "val l2_norm:  8115.751953125\n",
      "********** epoch:  97 **********\n",
      "train l2_norm:  5947.62382368608\n",
      "val l2_norm:  8050.966389973958\n",
      "********** epoch:  98 **********\n",
      "train l2_norm:  5924.78515625\n",
      "val l2_norm:  8033.168863932292\n",
      "********** epoch:  99 **********\n",
      "train l2_norm:  5877.652987393466\n",
      "val l2_norm:  8016.572672526042\n",
      "********** epoch:  100 **********\n",
      "train l2_norm:  5849.161709872159\n",
      "val l2_norm:  8089.745686848958\n",
      "********** epoch:  101 **********\n",
      "train l2_norm:  5861.088423295455\n",
      "val l2_norm:  8134.010579427083\n",
      "********** epoch:  102 **********\n",
      "train l2_norm:  5910.434636896307\n",
      "val l2_norm:  8233.936442057291\n",
      "********** epoch:  103 **********\n",
      "train l2_norm:  5914.971835049716\n",
      "val l2_norm:  7965.144694010417\n",
      "********** epoch:  104 **********\n",
      "train l2_norm:  5969.156605113636\n",
      "val l2_norm:  8060.812337239583\n",
      "********** epoch:  105 **********\n",
      "train l2_norm:  6001.003062855114\n",
      "val l2_norm:  7941.766194661458\n",
      "********** epoch:  106 **********\n",
      "train l2_norm:  5957.596213600852\n",
      "val l2_norm:  7992.111083984375\n",
      "********** epoch:  107 **********\n",
      "train l2_norm:  5960.028275923295\n",
      "val l2_norm:  7838.105794270833\n",
      "********** epoch:  108 **********\n",
      "train l2_norm:  5846.901766690341\n",
      "val l2_norm:  7917.169189453125\n",
      "********** epoch:  109 **********\n",
      "train l2_norm:  5779.89766068892\n",
      "val l2_norm:  7846.133056640625\n",
      "********** epoch:  110 **********\n",
      "train l2_norm:  5773.998557350852\n",
      "val l2_norm:  7839.179280598958\n",
      "********** epoch:  111 **********\n",
      "train l2_norm:  5771.08223100142\n",
      "val l2_norm:  7859.352864583333\n",
      "********** epoch:  112 **********\n",
      "train l2_norm:  5647.302889737216\n",
      "val l2_norm:  7960.164469401042\n",
      "********** epoch:  113 **********\n",
      "train l2_norm:  5594.764271129261\n",
      "val l2_norm:  8089.736572265625\n",
      "********** epoch:  114 **********\n",
      "train l2_norm:  5581.850408380682\n",
      "val l2_norm:  8023.32763671875\n",
      "********** epoch:  115 **********\n",
      "train l2_norm:  5710.108331853693\n",
      "val l2_norm:  8079.783610026042\n",
      "********** epoch:  116 **********\n",
      "train l2_norm:  5737.92333984375\n",
      "val l2_norm:  8131.763102213542\n",
      "********** epoch:  117 **********\n",
      "train l2_norm:  5722.751731178977\n",
      "val l2_norm:  8232.619303385416\n",
      "********** epoch:  118 **********\n",
      "train l2_norm:  5728.578391335227\n",
      "val l2_norm:  8129.186279296875\n",
      "********** epoch:  119 **********\n",
      "train l2_norm:  5658.833917791193\n",
      "val l2_norm:  7903.400309244792\n",
      "********** epoch:  120 **********\n",
      "train l2_norm:  5548.131946910511\n",
      "val l2_norm:  8085.133382161458\n",
      "********** epoch:  121 **********\n",
      "train l2_norm:  5498.385653409091\n",
      "val l2_norm:  7962.625\n",
      "********** epoch:  122 **********\n",
      "train l2_norm:  5492.08622602983\n",
      "val l2_norm:  8174.514322916667\n",
      "********** epoch:  123 **********\n",
      "train l2_norm:  5341.384011008523\n",
      "val l2_norm:  8249.214192708334\n",
      "********** epoch:  124 **********\n",
      "train l2_norm:  5267.990167791193\n",
      "val l2_norm:  8191.846435546875\n",
      "********** epoch:  125 **********\n",
      "train l2_norm:  5254.551225142045\n",
      "val l2_norm:  8365.605712890625\n",
      "********** epoch:  126 **********\n",
      "train l2_norm:  5242.380038174716\n",
      "val l2_norm:  8723.35009765625\n",
      "********** epoch:  127 **********\n",
      "train l2_norm:  5249.510187322443\n",
      "val l2_norm:  8313.451741536459\n",
      "********** epoch:  128 **********\n",
      "train l2_norm:  5330.395885120739\n",
      "val l2_norm:  8477.920735677084\n",
      "********** epoch:  129 **********\n",
      "train l2_norm:  5312.505415482955\n",
      "val l2_norm:  8351.817464192709\n",
      "********** epoch:  130 **********\n",
      "train l2_norm:  5497.257856889205\n",
      "val l2_norm:  8810.531901041666\n",
      "********** epoch:  131 **********\n",
      "train l2_norm:  5462.241122159091\n",
      "val l2_norm:  8333.956787109375\n",
      "********** epoch:  132 **********\n",
      "train l2_norm:  5398.158203125\n",
      "val l2_norm:  8175.848958333333\n",
      "********** epoch:  133 **********\n",
      "train l2_norm:  5444.38203568892\n",
      "val l2_norm:  8168.000081380208\n",
      "********** epoch:  134 **********\n",
      "train l2_norm:  5420.826815518466\n",
      "val l2_norm:  8324.719807942709\n",
      "********** epoch:  135 **********\n",
      "train l2_norm:  5441.338201349432\n",
      "val l2_norm:  8173.094889322917\n",
      "********** epoch:  136 **********\n",
      "train l2_norm:  5383.134832208807\n",
      "val l2_norm:  7964.28173828125\n",
      "********** epoch:  137 **********\n",
      "train l2_norm:  5300.122425426136\n",
      "val l2_norm:  7809.9619140625\n",
      "********** epoch:  138 **********\n",
      "train l2_norm:  5161.635364879261\n",
      "val l2_norm:  7719.201822916667\n",
      "********** epoch:  139 **********\n",
      "train l2_norm:  5088.839865944602\n",
      "val l2_norm:  7752.73095703125\n",
      "********** epoch:  140 **********\n",
      "train l2_norm:  4948.05859375\n",
      "val l2_norm:  7780.198079427083\n",
      "********** epoch:  141 **********\n",
      "train l2_norm:  4871.789595170455\n",
      "val l2_norm:  7878.735026041667\n",
      "********** epoch:  142 **********\n",
      "train l2_norm:  4821.328169389205\n",
      "val l2_norm:  7973.244466145833\n",
      "********** epoch:  143 **********\n",
      "train l2_norm:  4778.952903053977\n",
      "val l2_norm:  7976.728678385417\n",
      "********** epoch:  144 **********\n",
      "train l2_norm:  4749.9177357066765\n",
      "val l2_norm:  7974.764729817708\n",
      "********** epoch:  145 **********\n",
      "train l2_norm:  4740.146795099432\n",
      "val l2_norm:  7950.045817057292\n",
      "********** epoch:  146 **********\n",
      "train l2_norm:  4774.272927024148\n",
      "val l2_norm:  7971.735188802083\n",
      "********** epoch:  147 **********\n",
      "train l2_norm:  4757.899658203125\n",
      "val l2_norm:  8102.515869140625\n",
      "********** epoch:  148 **********\n",
      "train l2_norm:  4670.8825794566765\n",
      "val l2_norm:  8286.588704427084\n",
      "********** epoch:  149 **********\n",
      "train l2_norm:  4601.001886541193\n",
      "val l2_norm:  8093.749918619792\n",
      "********** epoch:  150 **********\n",
      "train l2_norm:  4566.333229758523\n",
      "val l2_norm:  8204.522705078125\n",
      "********** epoch:  151 **********\n",
      "train l2_norm:  4531.5340132279825\n",
      "val l2_norm:  8405.784342447916\n",
      "********** epoch:  152 **********\n",
      "train l2_norm:  4479.2423095703125\n",
      "val l2_norm:  8497.527262369791\n",
      "********** epoch:  153 **********\n",
      "train l2_norm:  4478.1459849964485\n",
      "val l2_norm:  8428.568766276041\n",
      "********** epoch:  154 **********\n",
      "train l2_norm:  4508.8662664240055\n",
      "val l2_norm:  8454.521809895834\n",
      "********** epoch:  155 **********\n",
      "train l2_norm:  4497.8687522194605\n",
      "val l2_norm:  8669.274251302084\n",
      "********** epoch:  156 **********\n",
      "train l2_norm:  4484.9213312322445\n",
      "val l2_norm:  8702.256022135416\n",
      "********** epoch:  157 **********\n",
      "train l2_norm:  4465.036221590909\n",
      "val l2_norm:  8621.764322916666\n",
      "********** epoch:  158 **********\n",
      "train l2_norm:  4478.930419921875\n",
      "val l2_norm:  8551.2900390625\n",
      "********** epoch:  159 **********\n",
      "train l2_norm:  4532.2266845703125\n",
      "val l2_norm:  8630.93603515625\n",
      "********** epoch:  160 **********\n",
      "train l2_norm:  4472.678200461648\n",
      "val l2_norm:  8642.666666666666\n",
      "********** epoch:  161 **********\n",
      "train l2_norm:  4453.587868430398\n",
      "val l2_norm:  8412.059244791666\n",
      "********** epoch:  162 **********\n",
      "train l2_norm:  4516.234841086648\n",
      "val l2_norm:  8508.836751302084\n",
      "********** epoch:  163 **********\n",
      "train l2_norm:  4572.0802223899145\n",
      "val l2_norm:  8237.812093098959\n",
      "********** epoch:  164 **********\n",
      "train l2_norm:  4721.17012162642\n",
      "val l2_norm:  8287.502115885416\n",
      "********** epoch:  165 **********\n",
      "train l2_norm:  4698.414040305398\n",
      "val l2_norm:  8119.818033854167\n",
      "********** epoch:  166 **********\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train l2_norm:  4682.912020596591\n",
      "val l2_norm:  8166.48095703125\n",
      "********** epoch:  167 **********\n",
      "train l2_norm:  4756.835293856534\n",
      "val l2_norm:  8013.414143880208\n",
      "********** epoch:  168 **********\n",
      "train l2_norm:  4788.939564098011\n",
      "val l2_norm:  8042.138916015625\n",
      "********** epoch:  169 **********\n",
      "train l2_norm:  4684.900213068182\n",
      "val l2_norm:  8360.08251953125\n",
      "********** epoch:  170 **********\n",
      "train l2_norm:  4374.6865789240055\n",
      "val l2_norm:  8393.172037760416\n",
      "********** epoch:  171 **********\n",
      "train l2_norm:  4168.429620916193\n",
      "val l2_norm:  8296.562255859375\n",
      "********** epoch:  172 **********\n",
      "train l2_norm:  4051.147804953835\n",
      "val l2_norm:  8282.5087890625\n",
      "********** epoch:  173 **********\n",
      "train l2_norm:  3994.7197043678975\n",
      "val l2_norm:  8296.1826171875\n",
      "********** epoch:  174 **********\n",
      "train l2_norm:  3963.3195578835225\n",
      "val l2_norm:  8242.079345703125\n",
      "********** epoch:  175 **********\n",
      "train l2_norm:  4000.3650235262785\n",
      "val l2_norm:  8209.20556640625\n",
      "********** epoch:  176 **********\n",
      "train l2_norm:  4067.4372225674715\n",
      "val l2_norm:  8262.519856770834\n",
      "********** epoch:  177 **********\n",
      "train l2_norm:  4109.3663552024145\n",
      "val l2_norm:  8498.822265625\n",
      "********** epoch:  178 **********\n",
      "train l2_norm:  4119.8339066938925\n",
      "val l2_norm:  8398.730794270834\n",
      "********** epoch:  179 **********\n",
      "train l2_norm:  4070.425526012074\n",
      "val l2_norm:  8339.887125651041\n",
      "********** epoch:  180 **********\n",
      "train l2_norm:  4019.191483931108\n",
      "val l2_norm:  8458.40576171875\n",
      "********** epoch:  181 **********\n",
      "train l2_norm:  3970.250577059659\n",
      "val l2_norm:  8247.0107421875\n",
      "********** epoch:  182 **********\n",
      "train l2_norm:  3946.098133433949\n",
      "val l2_norm:  8305.4208984375\n",
      "********** epoch:  183 **********\n",
      "train l2_norm:  3972.379638671875\n",
      "val l2_norm:  8654.788248697916\n",
      "********** epoch:  184 **********\n",
      "train l2_norm:  3966.0447221235795\n",
      "val l2_norm:  8789.055501302084\n",
      "********** epoch:  185 **********\n",
      "train l2_norm:  3986.2385475852275\n",
      "val l2_norm:  8519.911295572916\n",
      "********** epoch:  186 **********\n",
      "train l2_norm:  3971.1436101740055\n",
      "val l2_norm:  8696.9970703125\n",
      "********** epoch:  187 **********\n",
      "train l2_norm:  3968.667547052557\n",
      "val l2_norm:  8866.971516927084\n",
      "********** epoch:  188 **********\n",
      "train l2_norm:  3935.8799937855115\n",
      "val l2_norm:  8886.091145833334\n",
      "********** epoch:  189 **********\n",
      "train l2_norm:  3901.7174183238635\n",
      "val l2_norm:  8657.6904296875\n",
      "********** epoch:  190 **********\n",
      "train l2_norm:  3843.08925559304\n",
      "val l2_norm:  8887.794596354166\n",
      "********** epoch:  191 **********\n",
      "train l2_norm:  3857.3620383522725\n",
      "val l2_norm:  9140.826985677084\n",
      "********** epoch:  192 **********\n",
      "train l2_norm:  3967.5225830078125\n",
      "val l2_norm:  9213.527506510416\n",
      "********** epoch:  193 **********\n",
      "train l2_norm:  4072.033114346591\n",
      "val l2_norm:  9247.6884765625\n",
      "********** epoch:  194 **********\n",
      "train l2_norm:  4123.137295809659\n",
      "val l2_norm:  9043.865397135416\n",
      "********** epoch:  195 **********\n",
      "train l2_norm:  4064.2737260298295\n",
      "val l2_norm:  8772.2080078125\n",
      "********** epoch:  196 **********\n",
      "train l2_norm:  4081.184559215199\n",
      "val l2_norm:  9361.729817708334\n",
      "********** epoch:  197 **********\n",
      "train l2_norm:  4020.563620827415\n",
      "val l2_norm:  9244.068359375\n",
      "********** epoch:  198 **********\n",
      "train l2_norm:  3954.0888893821025\n",
      "val l2_norm:  8895.825520833334\n",
      "********** epoch:  199 **********\n",
      "train l2_norm:  3991.6105513139205\n",
      "val l2_norm:  8923.3955078125\n",
      "Minimum Valid Loss:  7719.201822916667\n"
     ]
    }
   ],
   "source": [
    "val_loss = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print('*'*10, 'epoch: ', epoch, '*'*10)\n",
    "    for phase in ['train', 'valid']:\n",
    "        if phase == 'train':\n",
    "            loss_list = []\n",
    "            model.train()\n",
    "            for i, data in enumerate(data_loaders[phase]):\n",
    "                input, label, depth = data\n",
    "                input = input.to(device)\n",
    "                label = label.to(device)\n",
    "                depth = depth.to(device)\n",
    "                output = model(input)\n",
    "                loss = l2_norm(output, label)\n",
    "                loss_list.append(loss.item())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            mean_loss = sum(loss_list) / len(loss_list)\n",
    "            print(\"train l2_norm: \", mean_loss)\n",
    "        elif phase == 'valid':\n",
    "            loss_list = []\n",
    "            model.eval()\n",
    "            for i, data in enumerate(data_loaders[phase]):\n",
    "                input, label, depth = data\n",
    "                input = input.to(device)\n",
    "                label = label.to(device)\n",
    "                depth = depth.to(device)\n",
    "                output = model(input)\n",
    "                loss = l2_norm(output, label)\n",
    "                loss_list.append(loss.item())\n",
    "            mean_loss = sum(loss_list) / len(loss_list)\n",
    "            print(\"val l2_norm: \", mean_loss)\n",
    "            val_loss.append(mean_loss)\n",
    "print('Minimum Valid Loss: ', min(val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 128, 128])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=128x128 at 0x7FFF7453BE50>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_pil(output[0][1].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
